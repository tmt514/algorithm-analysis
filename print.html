<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>演算法的分析與證明</title>
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">
        <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro:500" rel="stylesheet" type="text/css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        
        <link rel="stylesheet" href="styles/mdbook.css">
        

        
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    </head>
    <body class="light">
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { } 
            if (theme === null || theme === undefined) { theme = default_theme; }
            document.body.className = theme;
            document.querySelector('html').className = theme + ' js';
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <ol class="chapter"><li><a href="index.html">2020 新計劃</a></li><li><a href="GLOSSARY.html">Glossary</a></li><li><a href="sorting/index.html">排序 Sorting</a></li><li><ol class="section"><li><a href="sorting/bubble-sort.html">01/02 - 泡沫排序法</a></li><li><a href="sorting/cocktail-sort.html">01/03 - 雞尾酒排序法</a></li><li><a href="sorting/inversions.html">01/04 - 逆序數對</a></li><li><a href="sorting/merge-sort.html">01/05 - 合併排序法</a></li><li><a href="sorting/in-place-mergesort.html">01/06 - 原地演算法</a></li><li><a href="sorting/quick-sort.html">01/08 - 快速排序法</a></li><li><a href="sorting/randomized-quicksort.html">01/10 - 隨機快速排序法（一）</a></li><li><a href="sorting/randomized-quicksort2.html">01/12 - 隨機快速排序法（二）</a></li><li><a href="sorting/randomized-quicksort3.html">01/13 - 隨機快速排序法（三）</a></li><li><a href="sorting/heapsort.html">01/16 - 堆積排序法</a></li><li><a href="sorting/comparison-based-sorting.html">01/18 - 比較排序下界</a></li><li><a href="sorting/minimum-comparison-sort.html">01/20 - 最少比較排序</a></li><li><a href="sorting/merge-insertion-sort.html">01/22 - 合併插入排序</a></li><li><a href="sorting/poset-efficiency.html">01/25 - 偏序集排序（一）</a></li><li><a href="sorting/linear-extensions.html">01/26 - 偏序集排序（二）</a></li><li><a href="sorting/order-polytope-and-ehrhart-polynomial.html">01/27 - 偏序集排序（三）</a></li><li><a href="sorting/chain-polytope-and-graph-entropy.html">02/01 - 偏序集排序（四）</a></li><li><a href="sorting/poset-sort5.html">02/03 - 偏序集排序（五）</a></li></ol></li></ol>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar" class="menu-bar">
                    <div id="menu-bar-sticky-container">
                        <div class="left-buttons">
                            <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                                <i class="fa fa-bars"></i>
                            </button>
                            <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                                <i class="fa fa-paint-brush"></i>
                            </button>
                            <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                                <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                            </ul>
                            
                            <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                                <i class="fa fa-search"></i>
                            </button>
                            
                        </div>

                        <h1 class="menu-title">演算法的分析與證明</h1> 

                        <div class="right-buttons">
                            <a href="print.html" title="Print this book" aria-label="Print this book">
                                <i id="print-button" class="fa fa-print"></i>
                            </a>
                            
                        </div>
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <a class="header" href="#演算法的分析與證明" id="演算法的分析與證明"><h1>演算法的分析與證明</h1></a>
<p>這個是卡恩的 2020 新計劃。總之就是每天要寫一點東西這樣。
預計要放的東西，是在於演算法的正確性和時間複雜度的證明。
希望每一天的閱讀份量不超過五分鐘，然後有機會的話想順便偷渡一些這陣子看到的有趣演算法～</p>
<a class="header" href="#符號與說明" id="符號與說明"><h1>符號與說明</h1></a>
<a class="header" href="#時間複雜度" id="時間複雜度"><h2>時間複雜度</h2></a>
<p>對於一個演算法，考慮所有大小為 \(n\) 的輸入，若它的最差執行時間為 \(T(n)\)，那麼我們便說該演算法的最差時間複雜度為 \(T(n)\)。
通常我們會關心在 \(n\) 趨近無窮大的時候，這個函數的成長幅度為何。因此我們通常關心的是 \(T(n)\) 的漸進時間複雜度（Asymptotic Time Complexity）。
常見的漸進符號如下：</p>
<ul>
<li>Big-\(O\)：我們說一個演算法的執行時間為 \(O(f(n))\)，若存在一個常數 \(c &gt; 0\)、以及一個 \(n_0\in\mathbb{N}\)，使得對所有的 \(n &gt; n_0\) 皆有 \(T(n) \le cf(n)\)。</li>
<li>Big-\(\Omega\)：我們說一個演算法的執行時間為 \(\Omega(f(n))\)，若存在一個常數 \(c &gt; 0\)、以及一個 \(n_0\in\mathbb{N}\)，使得對所有的 \(n &gt; n_0\) 皆有 \(T(n) \ge cf(n)\)。</li>
<li>Big-\(\Theta\)：我們說一個演算法的執行時間為 \(\Theta(f(n))\)，若存在兩個常數 \(c_1, c_2 &gt; 0\)、以及一個 \(n_0\in\mathbb{N}\)，使得對於任意的 \(n &gt; n_0\) 都有 \(c_1f(n) \le T(n) \le c_2f(n)\)。</li>
</ul>
<a class="header" href="#計算模型" id="計算模型"><h2>計算模型</h2></a>
<p>最簡單的說法，是指計算模型嚴謹地定義了「\(O(1)\) 的時間可以做什麼」、以及「資料的存取方式」。</p>
<a class="header" href="#misc" id="misc"><h2>Misc</h2></a>
<ul>
<li>身為一個資訊系理論宅，我們使用的對數函數 \(\log\)，在沒有額外說明的情形下，一律以 2 為底。</li>
</ul>
<a class="header" href="#排序-sorting" id="排序-sorting"><h1>排序 Sorting</h1></a>
<p>排序是整理資料中一種最直接的方式。</p>
<a class="header" href="#基於比較的排序-comparison-based-sorting" id="基於比較的排序-comparison-based-sorting"><h2>基於比較的排序 Comparison Based Sorting</h2></a>
<table><thead><tr><th> 演算法 </th><th> 時間複雜度 </th><th> 空間複雜度 </th></tr></thead><tbody>
<tr><td> 泡沫排序法 </td><td> 最差 \(O(n^2)\), \(O(n + n\tau)\) </td><td> 最差 \(O(1)\) </td></tr>
<tr><td> 雞尾酒排序法 </td><td> 最差 \(O(n^2)\), \(O(n + n\sqrt{\tau})\) </td><td> 最差 \(O(1)\) </td></tr>
<tr><td> 插入排序法 </td><td> 最差 \(O(n^2)\), \(O(n + \tau)\) </td><td> 最差 \(O(1)\) </td></tr>
<tr><td> 選擇排序法 </td><td> 最差 \(O(n^2)\) </td><td> 最差 \(O(1)\) </td></tr>
<tr><td> 合併排序法 </td><td> 最差 \(O(n\log n)\) </td><td> 最差 \(O(n)\) </td></tr>
<tr><td> 原地合併排序 </td><td> 最差 \(O(n\log n)\) </td><td> 最差 \(O(1)\) </td></tr>
<tr><td> 快速排序法 - 第一個數字 pivot </td><td> 最差 \(O(n^2)\) </td><td> 最差 \(O(d) = O(n)\) </td></tr>
<tr><td> 快速排序法 - 隨機選 pivot </td><td> 期望 \(O(n\log n)\) </td><td> 期望 \(O(d) = O(\log n)\) </td></tr>
<tr><td> 快速排序法 - 找中位數 pivot </td><td> 最差 \(O(n\log n)\) </td><td> 最差 \(O(d) = O(\log n)\) </td></tr>
<tr><td> 原地快速排序 </td><td> 最差 \(O(n^2)\) </td><td> 最差 \(O(1)\) </td></tr>
<tr><td> 堆積排序法 </td><td> 最差 \(O(n\log n)\) </td><td> 最差 \(O(1)\) </td></tr>
</tbody></table>
<a class="header" href="#註記" id="註記"><h3>註記</h3></a>
<ul>
<li>\(n\) 代表輸入的資料量（序列元素個數）。</li>
<li>\(\tau\) 代表輸入序列的逆序數對個數。</li>
<li>\(d\) 代表遞迴深度。</li>
</ul>
<a class="header" href="#資料不改變存取位置的排序-data-oblivious-sorting" id="資料不改變存取位置的排序-data-oblivious-sorting"><h2>資料不改變存取位置的排序 Data-Oblivious Sorting</h2></a>
<table><thead><tr><th> 演算法 </th><th> 時間複雜度 </th><th> 排序網路深度 </th></tr></thead><tbody>
<tr><td> Shell 排序法 </td><td> 最差 \(O(n^2)\) </td><td> \(O(n)\) </td></tr>
<tr><td> Shell 排序法 - Pratt 版本 </td><td> 最差 \(O(n\log^2 n)\) </td><td> \(O(\log^2 n)\) </td></tr>
<tr><td> Batcher 雙調排序法 </td><td> 最差 \(O(n\log^2 n)\) </td><td> \(O(\log^2 n)\) </td></tr>
<tr><td> 奇偶合併排序 </td><td> 最差 \(O(n\log^2 n)\) </td><td> \(O(\log^2 n)\) </td></tr>
<tr><td> Ajtai-Komlós-Szemerédi 排序網路 </td><td> 最差 \(O(n\log n)\) </td><td> \(O(\log n)\) </td></tr>
<tr><td> Goodrich 排序網路 </td><td> 最差 \(O(n\log n)\) </td><td> \(O(n\log n)\) </td></tr>
</tbody></table>
<a class="header" href="#基於資料數值的排序-numbers-sorting" id="基於資料數值的排序-numbers-sorting"><h2>基於資料數值的排序 Numbers Sorting</h2></a>
<table><thead><tr><th> 演算法 </th><th> 時間複雜度 </th><th> 備註 </th></tr></thead><tbody>
<tr><td> 計數排序法 </td><td> \(O(n+M)\) </td><td> 正整數資料範圍 \(O(M)\) </td></tr>
<tr><td> \(k\)-進制桶子排序法 </td><td> \(O((n+k)\log_k \frac{1}{\epsilon})\) </td><td> 最相近兩數之相對誤差 \(\epsilon = \frac{\min_{i\neq j} \mid A[i]-A[j]\mid }{\max_i \mid A[i]\mid }\) </td></tr>
<tr><td> \(k\)-進制基數排序法 </td><td> \(O((n+k)\log_k M)\) </td><td> 正整數資料範圍 \(O(M)\) </td></tr>
</tbody></table>
<a class="header" href="#二進位整數排序-sorting-in-word-ram-model" id="二進位整數排序-sorting-in-word-ram-model"><h2>二進位整數排序 Sorting in Word RAM Model</h2></a>
<table><thead><tr><th> 演算法 </th><th> 時間複雜度 </th><th> 備註 </th></tr></thead><tbody>
<tr><td> van Emde Boas 樹 </td><td> \(O(n\log\log M)\) </td><td> 空間複雜度 \(O(M)\) </td></tr>
<tr><td> X-fast 字母樹 </td><td> \(O(n\log\log M)\) </td><td> 空間複雜度 \(O(n\log M)\) </td></tr>
<tr><td> Y-fast 字母樹 </td><td> \(O(n\log\log M)\) </td><td> 空間複雜度 \(O(n)\) </td></tr>
<tr><td> Fusion Tree </td><td> \(O(n\log n/\log\log n)\) </td><td> 允許隨機與除法可做到 \(O(n\sqrt{\log n})\) </td></tr>
<tr><td> Packed Sort </td><td> </td><td> </td></tr>
<tr><td> Signature Sort </td><td> </td><td> </td></tr>
<tr><td> Han-Thorup 整數排序 - 確定性 </td><td> \(O(n\log \log n)\) </td><td> 空間複雜度 \(O(n)\) </td></tr>
<tr><td> Han-Thorup 整數排序 - 隨機 </td><td> 期望 \(O(n\sqrt{\log \log n})\) </td><td> 空間複雜度 \(O(n)\) </td></tr>
</tbody></table>
<a class="header" href="#推薦閱讀" id="推薦閱讀"><h3>推薦閱讀</h3></a>
<ul>
<li>https://www.bigocheatsheet.com/</li>
<li>排序網路（維基百科）：https://en.wikipedia.org/wiki/Sorting_network</li>
<li>Shell 排序法（維基百科）：https://en.wikipedia.org/wiki/Shellsort</li>
</ul>
<a class="header" href="#泡沫排序法" id="泡沫排序法"><h1>泡沫排序法</h1></a>
<p>泡沫排序法 (Bubble Sort) 是一種利用氣泡往上面浮的概念所發明出來的排序方法。
不同大小的氣泡就像是會跟隔壁的人說「借過」一樣，利用不斷<strong>交換相鄰兩個元素</strong>來達到排序的效果。</p>
<p>假設我們有 \(n\) 筆資料。如果我們要實作泡沫排序法，我們可以重複掃描這個陣列，只要發現相鄰兩個數字的大小順序是錯誤的，便交換兩者。
一旦發覺陣列已排好序，就可以停止作業了。寫成程式碼長得像下面這樣：</p>
<pre><code class="language-cpp">// 泡沫排序法，呼叫完畢後 arr[] 內的元素將由小至大排列。
void BubbleSort(data_t arr[], int n) {
  while (AlreadySorted(arr, n) == false) {
    // 率先檢查陣列是否已排序。
    for (int i = 0; i + 1 &lt; n; i++) // (1)
      if (arr[i] &gt; arr[i + 1])      // (2)
        swap(arr[i], arr[i + 1]);   // (3)
  }
}
</code></pre>
<blockquote>
<p>實作上我們也可以利用一個 flag，紀錄在 while 迴圈中發生的事情：如果沒有任何交換的情事發生，則代表它已經排好序了。</p>
</blockquote>
<a class="header" href="#正確性的證明" id="正確性的證明"><h2>正確性的證明</h2></a>
<p>要怎麼證明呼叫 BubbleSort 以後，演算法能夠正確地將 <code>arr[]</code> 裡面的資料排好順序呢？
很顯然，當程式停下來的時候，<code>AlreadySorted</code> 會回傳 <code>true</code>，因此資料一定是排好順序的。
這是一個賴皮的證明方法，但它是對的。這也是為什麼在上述演算法我們用的是 while 而不是另一層 for-loop。
（但其實需要的核心證明是一樣的）
要怎麼證明演算法一定會停下來呢？這點我們可以留給時間複雜度一併分析。</p>
<a class="header" href="#時間複雜度-1" id="時間複雜度-1"><h2>時間複雜度</h2></a>
<p>我們可以利用數學歸納法證明以下敘述：</p>
<a class="header" href="#引理-1" id="引理-1"><h3>引理 1</h3></a>
<p>第 \(k\) 次執行 for-loop 之後，最大的 \(k\) 個元素已被正確地排入 <code>arr[n-k]...arr[n-1]</code>。</p>
<a class="header" href="#證明" id="證明"><h3>證明</h3></a>
<ul>
<li>Base Case: 當 \(k=0\) 的時候，我們不需要去排序。</li>
<li>Inductive Case: 當 \(k\ge 1\) 的時候，由於 <code>arr[n-k+1]...arr[n-1]</code> 都被放在正確的位置上，因此我們只要證明： <code>arr[0]...arr[n-k]</code> 當中最大的一筆資料會被置換到 <code>arr[n-k]</code> 的位置即可。怎麼證明這個敘述呢？注意到 for 迴圈 (1-3) 處的部分，假設最大的一筆資料目前在 <code>arr[j]</code>，而我們不妨假設 <code>arr[j]</code> 是所有相同值最右邊的那個。此時，因為 \(arr[j] \ge arr[j-1]\)，因此 \(i=j-1\) 的時候，(2) 不成立。此外，因為 \(arr[j] &gt; arr[j+1], \ldots, arr[n-k]\)，因此往後的每一次都會觸發 (3) 的交換操作，進而結論成立： <code>arr[j]</code> 被放在該放的位置上。</li>
</ul>
<p>如果輸入的資料已經排好順序了，那麼泡沫排序便只花費 \(O(n)\) 時間檢查陣列是否已排序。
根據引理 1，我們知道 while 迴圈只需要跑 \(O(n)\) 次。因此整體泡沫排序法的時間複雜度是 \(O(n^2)\)。</p>
<hr />
<p>這樣的時間複雜度分析真的是緊的嗎？是否真的有這麼差的情況發生？</p>
<a class="header" href="#雞尾酒排序法" id="雞尾酒排序法"><h1>雞尾酒排序法</h1></a>
<p>昨天我們介紹了泡沫排序法，但是，泡沫排序法最差情形 \(O(n^2)\) 的時間複雜度真的是緊的嗎？
讓我們看看以下例子。</p>
<a class="header" href="#引理-2" id="引理-2"><h3>引理 2</h3></a>
<p>對於這組輸入 \((a_1, a_2, \ldots, a_n) = (n, n-1, \ldots, 1)\)，泡沫排序法執行時間為 \(\Omega(n^2)\)。</p>
<a class="header" href="#證明-1" id="證明-1"><h3>證明</h3></a>
<p>我們可以由數學歸納法證得，第 \(k\) 次迴圈過後，陣列會變成 \([n-k, \ldots, 1, n-k+1, n-k+2, \ldots, n]\)。
因此，在 \(1\) 回到最前面之前，都不算排好順序。也就是說，泡沫排序法最差執行時間是 \(\Theta(n^2)\) 跑不掉的。</p>
<a class="header" href="#雞尾酒排序法-cocktail-sort" id="雞尾酒排序法-cocktail-sort"><h2>雞尾酒排序法 Cocktail Sort</h2></a>
<p>如果說由前往後，沒有辦法快速地把資料排好順序，那麼如果換個方式跑 for-loop，有沒有辦法變快呢？
<a href="https://zh.wikipedia.org/wiki/%E9%9B%9E%E5%B0%BE%E9%85%92%E6%8E%92%E5%BA%8F">雞尾酒排序法</a>，就是基於泡沫排序法進行修改而成的排序方法。它從左邊刷過去再從右邊刷回來：第一次把最大值放到最右邊去、然後把最小值推到最左邊、再把次大值放到右邊、再把次小值放到左邊…依此類推。
這個演算法的正確性證明與泡沫排序法相當雷同，因此時間複雜度也可以推得為 \(O(n^2)\)。
遺憾的是，引理 2 的例子也是這個演算法的最壞輸入之一：即在最壞情形下仍需 \(\Theta(n^2)\) 的執行時間。</p>
<a class="header" href="#插入排序法-insertion-sort" id="插入排序法-insertion-sort"><h2>插入排序法 Insertion Sort</h2></a>
<p>如果每一次都由右往左把資料刷回來，第 \(k\) 次把 \(arr[k-1]\) 從右邊往左邊一直推，
那麼概念上會變成：每一次加入一筆新的資料後，不斷地往前擠，插入到正確的位置上。這個方法也可以視為泡沫排序法的一個變形。
而事實上這個方法無論在現行的 CPU 架構下或是理論上，都不比泡沫排序法來得差。因此許多教科書都會選擇直接介紹插入排序法而非氣泡排序法。</p>
<a class="header" href="#引理-3-sup-classfootnote-referencea-href11asup" id="引理-3-sup-classfootnote-referencea-href11asup"><h3>引理 3 <sup class="footnote-reference"><a href="#1">1</a></sup></h3></a>
<p>如果輸入的資料中，每一筆資料距離其正確的位置皆不超過 \(k\) (\(k\ge 1\))，那麼泡沫排序法、雞尾酒排序法、以及插入排序法所需要的時間複雜度皆為 \(O(kn)\)。</p>
<hr />
<p>我們注意到，無論是氣泡排序法、雞尾酒排序法、或是插入排序法，這些方法在更動資料的時候，都只會交換相鄰兩筆資料。
而整體的時間複雜度，也至少是「交換兩筆資料」這個動作執行的次數。
對於一個陣列 \(A\)，我們可以定義只能交換相鄰兩筆資料的情形下，欲將 \(A\) 排好順序最少需要的交換次數。
這個數值，我們把它稱之為「逆序數對(Inversions)」。</p>
<!-- 如果更加推廣這個想法，我們不禁想問：對於一筆輸入，是否存在「最佳檢查順序」讓我們能夠順著這個順序交換資料，而且達到良好的時間複雜度呢？
<p>換句話說，對於一個輸入 \(A[1..n]\)，是否存在一個排列 \(\sigma: [n]\to [n]\)，使得以下演算法能夠很有效率地將資料排好順序？</p>
<ul>
<li><strong>while</strong>(還沒有排好順序):
<ul>
<li><strong>for</strong> \(i=1\) to \(n-1\):
<ul>
<li>如果 \(\sigma(i) &gt; \sigma(i+1)\) 和 \(A[\sigma(i)] &gt; A[\sigma(i+1)]\) 恰有一者不成立，則交換 \(A[\sigma(i)]\) 與 \(A[\sigma(i+1)]\) 之值。 --&gt;</li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="footnote-definition" id="1"><sup class="footnote-definition-label">1</sup>
<p><a href="https://en.wikipedia.org/wiki/Cocktail_shaker_sort">維基百科</a></p>
</div>
<a class="header" href="#逆序數對" id="逆序數對"><h1>逆序數對</h1></a>
<a class="header" href="#定義" id="定義"><h3>定義</h3></a>
<p>給定一個序列 \(A=(a_1, a_2, \ldots, a_n)\)。我們說一個數對 \((i, j)\), \(1\le i &lt; j\le n\) 是一個它的<strong>逆序數對 (Inversion)</strong>，若 \(a_i &gt; a_j\)。
而滿足條件的數對總數，我們寫作 \(\mathrm{Inv}(A)\)。</p>
<hr />
<p>有了這個定義以後，我們就可以討論當我們限制「每次只能交換相鄰兩數」的排序方法的效率了！</p>
<a class="header" href="#引理-4" id="引理-4"><h3>引理 4</h3></a>
<p>泡沫排序法、雞尾酒排序法、與插入排序法當中，交換數對的次數皆相同，而且恰好是 \(\mathrm{Inv}(A)\)。</p>
<a class="header" href="#證明-2" id="證明-2"><h3>證明</h3></a>
<p>由於交換的方式都是僅交換相鄰兩個數字，而且僅有在 \(a_i &gt; a_{i+1}\) 的時候才交換的。因此每一次交換後逆序數對總數恰好減 1。
此外，逆序數對數量為零，等價於序列已排好序。故得證。</p>
<a class="header" href="#係理-5" id="係理-5"><h3>係理 5</h3></a>
<p>插入排序法的時間複雜度為 \(O(n + \mathrm{Inv}(A))\)。（證明略）</p>
<a class="header" href="#係理-6" id="係理-6"><h3>係理 6</h3></a>
<p>存在一筆輸入，使得泡沫排序法的時間複雜度為 \(\omega(n + \mathrm{Inv}(A))\)。
即，對於該輸入，泡沫排序法需要 \(\Theta(n^2)\) 的時間，但是 \(\mathrm{Inv}(A)\) 很小，在本例中 \(\mathrm{Inv}(A) = O(n)\)。</p>
<a class="header" href="#證明-3" id="證明-3"><h3>證明</h3></a>
<p>該輸入為 \((2, 3, \ldots, n, 1)\)，此時要把 \(1\) 慢慢晃到最前面，需要歷經 \(n-1\) 次 while 迴圈。
注意到這個例子在雞尾酒排序法當中，仍是有效率的。</p>
<a class="header" href="#係理-7" id="係理-7"><h3>係理 7</h3></a>
<ol>
<li>泡沫排序法的時間複雜度為 \(O(n + n\cdot \mathrm{Inv}(A))\)，而且存在一些輸入符合這個時間複雜度。</li>
<li>雞尾酒排序法的時間複雜度為 \(O(n + n\cdot \sqrt{\mathrm{Inv}(A)})\)，而且存在一些輸入符合這個時間複雜度。</li>
</ol>
<a class="header" href="#證明-4" id="證明-4"><h3>證明</h3></a>
<p>第一部分還算單純：每一次 for-loop 之中，如果有 swap 就會讓逆序數對的數量減少 1。
因此泡沫排序法中 while 迴圈的執行次數至多是 \(\mathrm{Inv}(A)\) 次。由係理 6 也可以得知這個時間複雜度是緊的。</p>
<p>第二部分比較有意思，而且證明也相對複雜一些。注意到每次從左往右邊的 for-loop，拿來比較的 \(A[i]\) 是會越來越大的。
我們定義「上方階梯」為一個註標序列 \(1 = u_1 &lt; u_2 &lt; \ldots &lt; u_x\le n\)，代表每次由左往右邊看到的越來越大的當前極大值們。
不難發現在此階段的 swap 次數恰好就是 \(n-x\) 次。
同理我們可以定義「下方階梯」為另一個註標序列 \(1 \le d_y &lt; d_{y-1} &lt; \ldots &lt; d_1 = n\)，代表每次從右往左看到的越來越小的極小值們。
由於是雞尾酒排序法，從右往左刷的 swap 次數也恰好是 \(n-y\) 次。</p>
<p>我們的目標，是要證明每波操作以後，逆序數對總數至少會降 \(\sqrt{\mathrm{Inv}(A)}\) 這麼多。
由此可以推得至多 \(O(\sqrt{\mathrm{Inv}(A)})\) 次迴圈就可以把所有數值排好順序了。
如果 \(n-x=\Omega(n)\) 或 \(n-y=\Omega(n)\)，那麼顯然 \(n = \Omega(\sqrt{\mathrm{Inv}(A)})\)。</p>
<p>我們現在來研究逆序數對總數與 \(n-x, n-y\) 之間的關係。我們宣稱</p>
<p>\[
\mathrm{Inv}(A) \le (n-x)^2 + (n-y)(n-x) + (n-y)^2 \le ((n-x) + (n-y))^2\text{。}
\]
由上式可知，一個 while-loop 內來回刷過一次之後，發生的置換次數 \((n-x)+(n-y)\) 次至少有 \(\sqrt{\mathrm{Inv}(A)}\) 這麼多次！</p>
<p>要怎麼證明上面這個式子呢？首先可以將所有的逆序數對分成：(1) 兩個註標都不在上方階梯；(2) 兩個註標都不在下方階梯；或 (3) 一個註標在上方階梯、另一個註標在下方階梯。
要注意的是，不可能有兩個註標在上方階梯（或下方階梯）的情形——因為階梯本身就不會出現逆序數對。
此外，也不可能有一個註標同時出現在上方階梯與下方階梯、並且另一個註標同時不出現在階梯上——這是因為，若一個註標同屬於上下階梯的話，不可能產生與之相關的逆序數對。
因此，(1) 可以得出 \((n-x)^2\)，(2) 可以得出 \((n-y)^2\)，(3) 由於註標不能同時出現在兩階梯，因此可以得到上界 \((n-y)(n-x)\)。</p>
<p>考慮函數 \(f(k) = k-\sqrt{k}\)，我們想要知道的就是迭代多少次以後，\(f(\cdots f(f(k))\cdots)\) 的值會變得夠小。
令 \(k=t^2\)。
由於 \(t^2-t \ge (t-1)^2\)。
注意到迭代兩次以後 \(f(f(k)) = k-\sqrt{k}-\sqrt{k-\sqrt{k}} \le t^2 - t - (t-1) = (t-1)^2\)。
因此，只要迭代 \(O(t)\) 次以後，就可以回到常數囉。因此，需要的 while-loop 迭代次數至多只要 \(O(\sqrt{\mathrm{Inv}(A)})\) 次。</p>
<hr />
<p>什麼樣的輸入需要讓雞尾酒排序法花到 \(\Theta(n+n\sqrt{\mathrm{Inv}(A)})\) 這麼久呢？
可以試試看把 \(1\) 到 \(n\) 依序排好，然後把前 \(k\) 個數字反轉過來，你就會得到一個逆序數對個數為 \(k(k-1)/2\)、而且時間複雜度是 \(O(nk)\) 的結論了。</p>
<a class="header" href="#小結論" id="小結論"><h2>小結論</h2></a>
<p>我們今天透過逆序數對的方法來分析氣泡排序法、雞尾酒排序法以及插入排序法的優劣。</p>
<a class="header" href="#參考資料" id="參考資料"><h3>參考資料</h3></a>
<ul>
<li>https://en.wikipedia.org/wiki/Inversion_(discrete_mathematics)</li>
<li>https://math.stackexchange.com/questions/2998546/sort-an-array-given-the-number-of-inversions</li>
</ul>
<a class="header" href="#合併排序法" id="合併排序法"><h1>合併排序法</h1></a>
<p>如果我們不限制每次都交換相鄰兩個數字的話，顯然存在著理論上有更有效率的排序演算法。</p>
<p>分而治之（Divide and Conquer）是重要的解題方法之一：我們將一個大問題拆解成許多小問題、個個擊破以後再把它們組合回來。
如果我們將分而治之法應用在排序問題當中，就可以得到俗稱合併排序法（Merge Sort）的演算法：</p>
<ul>
<li>假設欲排序 \(A[0..n-1]\)。
<ol>
<li>把輸入的資料拆成差不多相等的兩半：\(L=A[0..\lfloor\frac{n}{2}\rfloor-1]\)、\(R=A[\lfloor\frac{n}{2}\rfloor..n-1]\)。</li>
<li>呼叫遞迴將兩邊分別排序。</li>
<li>假設 \(L\) 與 \(R\) 已經各自排好順序，將 \(L\) 與 \(R\) 合併回完整的、排好順序的資料陣列 \(A\)。</li>
</ol>
</li>
</ul>
<p>實作上大概長得像這樣：</p>
<pre><code class="language-cpp">void MergeSort(data_t *A, int n) {
  // 只剩一個元素的時候，整個陣列就已經排好順序啦。
  if (n &lt;= 1) return;
  // Divide &amp; Conquer 的各自遞迴部分。
  MergeSort(A, n/2);
  MergeSort(A + n/2, n - n/2);
  // Combine: 這邊是仿效 C++ STL 的 std::merge() 格式。
  Merge(A, A + n/2, A + n/2, A + n, A);
}
</code></pre>
<a class="header" href="#合併兩個序列" id="合併兩個序列"><h2>合併兩個序列</h2></a>
<p>要怎麼將兩個排好順序的序列合併起來呢？一個簡單的想法是：每一次考慮兩個序列目前的排頭，把比較小的那個拉出來，最終接成一串。
如果用 linked list 或 vector 寫起來大概像這樣：</p>
<pre><code class="language-cpp">void Merge(data_t *L_start, data_t *L_end,
           data_t *R_start, data_t *R_end, data_t *Output) {
  vector&lt;data_t&gt; tmp;
  data_t *L_ptr = L_start, *R_ptr = R_start;
  while (L_ptr != L_end || R_ptr != R_end) {
    // 把比較小的那筆資料複製到 vector 裡面。
    if (L_ptr != L_end &amp;&amp; (R_ptr == R_end || *L_ptr &lt; *R_ptr))  // (**)
      tmp.push_back(*L_ptr++);
    else
      tmp.push_back(*R_ptr++);
  }
  // 再把合併好的資料抄回去。
  for (const auto &amp;data : tmp) *Output++ = data;
}
</code></pre>
<p>不難發現，由於兩個指標 <code>*L_ptr</code> 和 <code>*R_ptr</code> 不斷遞增，而且每次迴圈<strong>恰好</strong>有一者前進一格。
因此整個 <code>Merge()</code> 的時間複雜度是線性的 \(\Theta(n)\)。
正確性來自於對排序資料的假設、以及每一次我們都拉比較小的排頭出來，這個排頭必定是剩下所有還沒被排進來的資料中，最小的一個。</p>
<p>近幾年這個合併兩個序列的方法已經被歸類成一種叫做 Two Pointers 的技巧。當然這個技巧當初是用來描述<a href="https://en.wikipedia.org/wiki/Cycle_detection">龜兔賽跑方法</a>（這個用來找 Linked List 尾圈方法我們以後再聊），但已經被推廣成泛指「只要維護兩個註標，這兩個註標只會往單方向移動」的演算法。</p>
<a class="header" href="#時間複雜度的分析" id="時間複雜度的分析"><h2>時間複雜度的分析</h2></a>
<p>由於每一次的資料搬移，大致可以對應到 (**) 處的比較，因此合併排序法所需要的時間，我們可以用「兩兩比較次數」代表之。
現在我們來證明合併排序法的時間複雜度是 \(O(n\log n)\)。</p>
<a class="header" href="#引理-8" id="引理-8"><h3>引理 8</h3></a>
<p>令 \(T(n)\) 表示以合併排序法排序 \(n\) 個數字時，所需要進行的比較次數。則 \(T(n) = O(n\log n)\)。</p>
<a class="header" href="#證明-5" id="證明-5"><h3>證明</h3></a>
<p>首先，當 \(n\) 是 \(2\) 的冪次（即 \(n\) 形如 \(2^k\) 時），兩個子問題的大小都是 \(n/2\)。此時我們有：</p>
<p>\[
T(n) = \begin{cases} 0 &amp; \text{ if } n\le 1, \\
2T(\frac{n}{2}) + n-1 &amp; \text{ if } n &gt; 1. \end{cases}
\]</p>
<p>然後根據<a href="https://zh.wikipedia.org/zh-tw/%E4%B8%BB%E5%AE%9A%E7%90%86">主定理（Master Theorem）</a>或是數學歸納法，不難得出 \(T(n) = \Theta(n\log n)\) 的結論。</p>
<p>當 \(n\) 不是 \(2\) 的冪次時（這很重要！）上面這個時間複雜度的遞迴式其實不全然正確。
事實上，遞迴式應該是這樣才對：</p>
<p>\[
T(n) = T(\floor{\frac{n}{2}}) + T(\ceil{\frac{n}{2}}) + n-1
\]</p>
<p>好加在，我們可以利用數學歸納法證明出 \(T(n)\) 是遞增的。換句話說，令 \(n' = 2^{\ceil{\log n}}\)，利用 \(n \le n' &lt; 2n\)，我們可以得到</p>
<p>\[
T(n) \le T(n') = \Theta(n' \log n') = \Theta(n\log n)\text{。}
\]</p>
<hr />
<p>合併排序法在合併的時候，會需要<strong>複製</strong>欲排序的資料。因此，合併排序法在實作的時候，一般來說會佔用到 \(\Theta(n)\) 的額外記憶體空間。
我們有辦法省下些許額外的記憶體嗎？</p>
<a class="header" href="#推薦閱讀-1" id="推薦閱讀-1"><h3>推薦閱讀</h3></a>
<ul>
<li>主定理的天花板形式：https://fu-sheng-wang.blogspot.com/2016/11/algorithms14-master-theorem.html</li>
</ul>
<a class="header" href="#原地演算法" id="原地演算法"><h1>原地演算法</h1></a>
<p>合併排序法有著許多優點：
<strong>時間複雜度</strong>勝過泡沫排序法、選擇排序法；
<strong>執行效能</strong>相較快速排序法更穩定一些；
<strong>記憶體存取</strong>相較於堆積排序法更連續一些；
甚至因為遞迴方式穩定，可以根據硬體特性特化出許多專門的晶片來處理小數列排序、平行排序等等，也可以讓編譯器預先優化。</p>
<p>當然，關心理論的你，可能會發現，在實作合併兩個排好序的陣列時，總是得開一塊新的記憶體空間，然後把資料騰過去<sup class="footnote-reference"><a href="#1">1</a></sup>，然後合併完再騰寫回來。
感覺很浪費記憶體啊。
如果我們有辦法就地移動資料本身，不仰賴大量額外的記憶體的話，這類型的演算法通常被稱為<strong>原地演算法（In-Place Algorithms）</strong>。
最嚴格的原地演算法定義，是規定只能使用<strong>常數</strong><sup class="footnote-reference"><a href="#4">2</a></sup>數量的記憶體空間（用來存放註標、或某些計數器等等資料）。</p>
<a class="header" href="#性質-9" id="性質-9"><h3>性質 9</h3></a>
<p>泡沫排序法、雞尾酒排序法、插入排序法、選擇排序法都是原地演算法。</p>
<hr />
<p>要怎麼在沒有額外記憶體空間的情況下，做到合併功能呢？今天我們會介紹在 Stack overflow<sup class="footnote-reference"><a href="#3">3</a></sup> 上面找到的、 Katajianen-Pasanen-Teuhola <sup class="footnote-reference"><a href="#2">4</a></sup> 把合併排序法和插入排序法融合起來，得到的原地排序演算法。其核心想法是由蘇聯數學家 <a href="https://en.wikipedia.org/wiki/Alexander_Kronrod">Kronrod</a> 在 1969 年提出的「從輸入陣列榨出暫存空間」的方法。</p>
<a class="header" href="#kronrod-工作空間重疊大法" id="kronrod-工作空間重疊大法"><h2>Kronrod 工作空間重疊大法</h2></a>
<p>假設我們有兩塊相鄰的陣列區段 \(A\) 和 \(B\)，如下圖：</p>
<p><img src="./in-place-mergesort1.png" alt="" /></p>
<p>注意到 \(A\) 左邊有一塊可以暫時拿來當作暫存空間用的未排序區域，但如果它的長度比 \(|A\mid +\mid B\mid \) 來得小，很有可能在合併排序的過程中或覆蓋到 \(A\) 區域的資料。可能就會慘掉。</p>
<p><img src="./in-place-mergesort2.png" alt="" /></p>
<a class="header" href="#引理-10-kronrod" id="引理-10-kronrod"><h3>引理 10 (Kronrod)</h3></a>
<p>Kronrod 注意到，假設 \(|A\mid \) 的長度比 \(|B\mid \) 短。如果我們將 \(A\) 搬到左邊，使得 \(A\) 與 \(B\) 之間空出 \(|A\mid \) 個暫存空間。
這個時候，我們就可以安心地使用這塊空間當作合併排序的暫存空間了！就算合併時碰到了原本屬於 \(B\) 區段的空間也沒有關係，因為可以保證不會重疊到 \(B\) 目前的工作區間。（證明略）</p>
<p>使用這個想法，我們可以得出像是 Katajianen-Pasanen-Teuhola 的原地演算法。</p>
<a class="header" href="#katajianen-pasanen-teuhola-的原地類合併排序法" id="katajianen-pasanen-teuhola-的原地類合併排序法"><h2>Katajianen-Pasanen-Teuhola 的原地類合併排序法</h2></a>
<p>假設輸入的陣列為 \(A[0..n-1]\) 其中 \(n=2^k\)。這個排序演算法總共分成三個部分。</p>
<ul>
<li>第一部分，首先我們排好整個陣列的後半部 \(A[\frac{n}{2}..n-1]\)。由於我們可以利用前半部當作暫存空間，所以不用擔心空間不夠。</li>
<li>第二部分，依序對於 \(\ell = k-2, k-3, \ldots, 0\)：
<ul>
<li>開始前我們知道 \(A[0..2^{\ell+1}-1]\) 是未排序部分、然後其他部分是已排序的。</li>
<li>我們試圖排序前 \(2^{\ell}\) 個元素、並且合併到排好序的 \(A[2^{\ell+1}..n-1]\) 部分。
<ul>
<li>排序時，可以利用中段作為暫存空間。</li>
<li>注意到中間的空位剛好有 \(2^{\ell}\) 格，因此可以使用引理 10 維護其合併的正確性。</li>
</ul>
</li>
<li>合併完成後，未排序元素的位置恰好被換到 \(A[0..2^{\ell}-1]\)。可以接續下一個循環。</li>
</ul>
</li>
<li>第三部分，把剩下一個元素 \(A[0]\) 使用插入排序法排入 \(A[1..n-1]\) 之中。</li>
</ul>
<p><img src="./in-place-mergesort3.png" alt="" /></p>
<a class="header" href="#定理-11" id="定理-11"><h3>定理 11</h3></a>
<p>KPT-類合併排序法的時間複雜度為 \(O(n\log n)\)、而且輸入陣列以外的空間複雜度為 \(O(1)\)。</p>
<a class="header" href="#證明-6" id="證明-6"><h3>證明</h3></a>
<p>空間複雜度應該還滿…顯然的吧XD，我們來證明時間複雜度。</p>
<p>正常的合併排序時間複雜度是 \(O(n\log n)\)。因此第一部份的時間複雜度就是 \(O(\frac{n}{2}\log \frac{n}{2}) = O(n\log n)\)。
對於每個 \(\ell\)，第二部份會花 \(O(2^\ell\log 2^\ell)\) 時間進行合併排序、然後花費 \(\frac{n}{2} + 2^\ell = O(n)\) 的時間進行合併。
所以，第二部份的每一個迴圈 \(\ell\) 的時間複雜度是 \(O(n+2^\ell\log 2^\ell)\)。
全部加起來，就會得到</p>
<p>\[
\sum_{\ell=0}^{(\log n)-2} (n+2^{\ell}\log 2^\ell)
\le n\log n + (2^0+2^1+\cdots + 2^{(\log n)-2})\log n = O(n\log n)\text{。}
\]</p>
<p>第三部分時間複雜度是 \(O(n)\)。所以全部加起來是 \(O(n\log n)\)，得證。</p>
<hr />
<a class="header" href="#後記" id="後記"><h2>後記</h2></a>
<p>從 KPT-之後，大部分的論文朝向幾個方向發展。大家關心的分析重點有幾個方向：把時間複雜度分拆成「比較次數」加上「資料移動次數」。
比較次數雖然都是 \(O(n\log n)\)，但大家開始拼命壓前面的常數。而資料移動次數也有<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.22.5514&amp;rep=rep1&amp;type=pdf">不同方法</a>可以壓到 \(O(n\log n/ \log\log n)\)。
此外，我們其實利用的分而治之的概念，偷偷榨出了不少暫存空間。如果我們回歸到最原本的問題：合併兩個已排序的序列，是否真的能在線性時間做到不使用額外空間呢？答案是可以的，但我想今天就在此打住吧。有興趣的朋友可以參考<a href="https://academic.oup.com/comjnl/article/38/8/681/335248">這篇</a>。</p>
<a class="header" href="#參考資料-1" id="參考資料-1"><h3>參考資料</h3></a>
<ul>
<li>清大韓永楷教授的 In-Place Algorithms 簡介投影片：http://www.cs.nthu.edu.tw/~wkhon/algo08-tutorials/tutorial1b.pdf</li>
<li>嘗試優化你的合併排序法（減少 Branch Prediction）：https://www.codeplay.com/portal/optimizing-sort-algorithms-for-the-ps3-part-2-merge</li>
</ul>
<!-- * 交大黃光明教授和 S. Lin 當年在貝爾實驗室發表的合併演算法：https://epubs.siam.org/doi/abs/10.1137/0201004 -->
<div class="footnote-definition" id="1"><sup class="footnote-definition-label">1</sup>
<p>實作上如果資料本身很大，通常也可以對「註標」進行排序就好。這樣雖然可以避免搬移資料，但是在比較大小時會產生一些記憶體隨機存取的操作，可能會產生出很多 Cache Miss。
<sup class="footnote-reference"><a href="#2">4</a></sup>: <a href="https://pdfs.semanticscholar.org/9de9/2ae68f76c040c40fad4bb3aabb7146cb8c3d.pdf">Practical In-Place Mergesort, J. Katajianen, T. Pasanen, and J. Teuhola, Nordic Journal of Computing 1996.</a>
<sup class="footnote-reference"><a href="#3">3</a></sup>: https://stackoverflow.com/questions/2571049/how-to-sort-in-place-using-the-merge-sort-algorithm
<sup class="footnote-reference"><a href="#4">2</a></sup>: 這裡的「常數」要仰賴計算模型而定。一般來說為了支援隨機記憶體存取(RAM Model)，我們必須讓一單位的記憶體放得下對應資料的記憶體位址。也就是說，當輸入有 \(n\) 筆資料時，原地演算法允許我們使用 \(O(\log n)\)-bit 的額外記憶體空間。</p>
</div>
<a class="header" href="#快速排序法" id="快速排序法"><h1>快速排序法</h1></a>
<p>另一個利用分而治之達成的排序方法，是快速排序法（Quick Sort）。</p>
<p>如果我們能將輸入的數值分成兩組，並且保證某一組的所有數字都不超過另一組的所有數字。
那麼，把這兩組數字分別遞迴下去排序，再接起來就可以了。
通常分組的方法是透過指定衛兵（pivot）的形式，比衛兵小的放一堆、比衛兵大的放另一堆。</p>
<p>分組的方法非常多種，這裡的寫法是直接指定<strong>第一個數字為 pivot</strong>，然後逐一掃過所有元素。
如果一個元素比 pivot 大的話，就把它換到最後面，否則就跟 pivot 交換（放到左邊）。
咦？如果有跟衛兵相同的怎麼辦？基本上我們必須排除與衛兵相同的資料值，所以遞迴的時候必須要把小於、等於、大於這三種情況分清楚。</p>
<pre><code class="language-cpp">void QuickSort(data_t *A, int n) {
  if (n &lt;= 1) return;
  int pivot_pos = 0;
  int right_boundary = n - 1;
  int now = 1;
  // 把資料分成小於、等於、大於 pivot 等三堆。
  while (now &lt;= right_boundary) {
    if (A[now] &lt; A[pivot_pos]) {
      swap(A[now++], A[pivot_pos++]);
    } else if (A[now] == A[pivot_pos]) {
      ++now;
    } else {
      swap(A[now], A[right_boundary--]);
    }
  }
  QuickSort(A, pivot_pos); // 嚴格小於 pivot 的元素們。
  QuickSort(A + now, n - now); // 嚴格大於 pivot 的元素們。
}
</code></pre>
<a class="header" href="#最差時間複雜度" id="最差時間複雜度"><h2>最差時間複雜度</h2></a>
<a class="header" href="#引理-12" id="引理-12"><h3>引理 12</h3></a>
<p>快速排序法的時間複雜度為 \(O(n^2)\)。</p>
<a class="header" href="#證明-7" id="證明-7"><h3>證明</h3></a>
<p>注意到，對於任何一筆資料，在所有 QuickSort 函式的呼叫中最多只有一次會被當作 pivot 拿來比較。
而 QuickSort 的執行時間正比於：\(n\le 1\) 情形的遞迴呼叫次數、以及拿 pivot 出來跟別人比較的總次數。
前者是 \(O(n)\) 次函式呼叫，後者每個元素至多被拿來當作一次 pivot，每次 pivot 至多會跟 \(O(n)\) 個元素進行比較。
因此時間複雜度為 \(O(n^2)\)。</p>
<p>另一方面，考慮 \((n, n-1, n-2, \ldots, 1)\) 這個逆序的序列。每一次遞迴都會把最大的數字推到最右邊，因此每一層遞迴要處理的資料量依序為 \(n, n-1, \ldots, 1\)。這樣加起來就是 \(\Theta(n^2)\)。</p>
<a class="header" href="#快速排序法不是原地演算法" id="快速排序法不是原地演算法"><h3>快速排序法不是原地演算法</h3></a>
<p>雖然上面把資料分堆的部分，看起來只需要 \(O(1)\) 的記憶體空間。但遞迴呼叫會在使用呼叫堆疊（Call Stack），在最糟情形下仍會使用 \(O(n)\) 的額外記憶體空間。相當慘烈。此外，要把快速排序法改成非遞迴的形式，其實就有點痛苦了，不像是合併排序法那樣可以輕易地變成 bottom-up 的迭代形式。</p>
<a class="header" href="#Ďurian-快速排序法" id="Ďurian-快速排序法"><h2>Ďurian 快速排序法</h2></a>
<p>一般來說，把一個遞迴函式無腦改成非遞迴函式，最簡單的方式就是手動模擬呼叫堆疊的行為。
但事實上，今天來跟大家分享一個捷克學生 Branislav Ďurian 在 1986 年提出的改編版快速排序法。
其核心想法是利用 pivot 當作擋板，找出目前遞迴到的子問題邊界。這麼一來，就不需要使用堆疊來存放邊界了！</p>
<p><img src="./in-place-quicksort1.png" alt="" /></p>
<p>上圖示意著原本的快速排序法的遞迴呼叫過程，其中 L 和 R 表示的是當下的子問題邊界、而藍色和紅色底的兩條代表著兩次遞迴所選取的 pivot。
大家可以想想看，如果不知道邊界的情形下（或者，只記得剛才處理的子問題邊界），要怎麼找回 pivot。
如果是已分派好的 pivot，其實扮演著<strong>樞紐</strong>的功能：在它左邊的元素都比較小、在它右邊的元素都比較大。
所以其實可以用線性時間掃過去，花費 \(O(n)\) 的時間來找出樞紐。（要做到線性時間+常數額外記憶體需要一點點挑戰性，朋友們不妨思考看看）
不過這麼一來，就算我們在「遞迴的底層」（資料規模很小的子問題）仍然需要花費 \(O(n)\) 的時間找出樞紐，沒什麼效率。</p>
<p>相反地，如果我們只花 \(O(R-L)\) 的時間找出下一個要處理的子問題，整體的時間就不會變了！
改變目前遞迴子問題的情況只有兩種：一個是往下遞迴（比 pivot 小的那一組數字）、另一個是在遞迴結束後，需要移動到上一層（或上好幾層）的比 pivot 大的那一組數字。第一種情形相當好處理，在往下遞迴之前你已經有目前 pivot 的位置了！</p>
<p>難就難在第二種情形。一個子問題搞定以後，如果要繼續排序，下一個要搞定的子問題總是會粘著目前子問題的右界。
換句話說，我們其實已經知道下一個要處理的子問題左界在哪裡了！
要怎麼做到還原右邊界這件事呢？Ďurian 提出了一個絕妙的方法：在向下遞迴之前，偷偷把<font color='red'><strong>前一層的 pivot</strong> 換到目前 pivot 右邊</font>。
以下圖為例，我們把藍色 pivot 調到紅色 pivot 右邊！</p>
<p><img src="./in-place-quicksort2.png" alt="" /></p>
<p>當我們做完右圖的 \([L, R]\) 子問題以後，要怎麼還原藍色 pivot 的正確位置呢？
由於已知藍色 pivot 的位置，這時候就變得相當單純了！我們就拿著這個藍色 pivot 往右邊刷，刷到第一個不比它小的數字就停下來。
這麼一來就可以準確還原右界了！而且不需要任何堆疊或額外的記憶體空間！</p>
<hr />
<p>快速排序法的最差情形是 \(O(n^2)\)，乍看之下很沒有效率啊。
但其實這個演算法相當有用啊，不致於這麼差吧──大家平常在說的快速排序法可以做到 \(O(n\log n)\) 是怎麼一回事？</p>
<a class="header" href="#參考資料-2" id="參考資料-2"><h3>參考資料</h3></a>
<ul>
<li>郭至軒的（厲害而詳盡的）筆記：https://blog.kuoe0.tw/posts/2013/03/15/sort-about-quick-sort/</li>
<li>非遞迴、不堆疊的快速排序法：https://link.springer.com/chapter/10.1007/BFb0016252</li>
</ul>
<a class="header" href="#隨機快速排序法" id="隨機快速排序法"><h1>隨機快速排序法</h1></a>
<p>如果我們允許演算法使用隨機數產生器，那麼快速排序法會變得更有效率！</p>
<!--
## 隨機演算法
一般的確定性（deterministic）演算法 \\(\mathcal{A}(x)\\)，可以把它看成一個函數，其參數就是輸入的資料。
而一個使用隨機數產生器的演算法 \\(\mathcal{A}(x, r)\\)，我們可以把隨機數產生器的內容 \\(r\\) 當作「額外的輸入參數」。
當演算法需要使用新的隨機數字的時候，就從這個額外的輸入抓取新的數字來使用。
通常在討論隨機演算法的時候，我們會假設產生出來的隨機數字 \\(r\\) 的每一個位元（或每一個數值，依照計算模型而定）都是均勻分布的。
## 期望複雜度
在考慮使用隨機數產生器的時候，我們自然不會過於在意某幾個造成演算法執行時間過久的隨機數值。
而此時演算法的執行時間 \\(T(n)\\)，就會變成一個隨機變數。分析的時候，我們傾向於找出這個隨機演算法的
-->
<p>而引入隨機的方法很簡單：我們只要在快速排序法開始之前，隨機排列原本的輸入序列即可！</p>
<pre><code class="language-cpp">void RandomizedQuickSort(data_t *A, int n) {
  RandomPermute(A, n); // 隨機排列輸入的數值。
  QuickSort(A, n);     // 呼叫原本的快速排序法。
}
</code></pre>
<a class="header" href="#定理-13" id="定理-13"><h2>定理 13</h2></a>
<p>假設 <code>RandomPermute</code> 可以均勻地產生隨機排列，那麼隨機快速排序法的期望時間複雜度為 \(O(n\log n)\)。</p>
<a class="header" href="#開始證明之前" id="開始證明之前"><h3>開始證明之前</h3></a>
<p>大家還記不記得<a href="https://zh.wikipedia.org/wiki/%E6%9C%9F%E6%9C%9B%E5%80%BC">期望值的可加性</a>呢？簡單來說，如果有兩個隨機變數（Random Variable） \(X, Y\)，那麼 \(X+Y\) 的期望值便滿足
\(\E[X+Y] = \E[X] + \E[Y]\)。我們把這個觀念應用在分析演算法的期望時間複雜度上面，可以把它想成：若這個演算法分成前後兩個步驟 \(A\) 和 \(B\)，兩個步驟的執行時間分別可以用隨機變數 \(T_A\) 與 \(T_B\) 表示。那麼整個演算法的執行時間就可以寫成 \(T_A+T_B\)，而這個隨機變數的期望值就可以利用期望值的可加性寫得 \(\E[T_A+T_B] = \E[T_A]+\E[T_B]\)。也就是說，我們只要分別分析兩個步驟的期望執行時間，加起來，就會等於整個演算法的期望時間複雜度了。</p>
<a class="header" href="#證明一遞迴方法" id="證明一遞迴方法"><h3>證明一：遞迴方法</h3></a>
<p>我們定義 \(f(n)\) 為對 \(n\) 筆資料進行隨機快速排序的期望時間複雜度。我們想證明的是 \(f(n) = O(n\log n)\)。
首先，我們可以簡單地說明 \(f(n)\) 是非遞減的：多一筆資料，要排好序總得花更多力氣。
根據快速排序法的定義，我們選擇第一個數字作為 pivot，然後把整組資料拆成小於、等於、大於三個部分，再把小於 pivot、大於 pivot 這兩組資料分別遞迴下去排序。</p>
<p>由於一開始我們就把陣列中所有數字都打亂了，所以作為 pivot 的「第一個數字」其實是所有資料皆以均等的機率出現的。
因此，我們有 \(1/n\) 的機率選中最小值作為 pivot、有 \(1/n\) 的機率選中次小值作為 pivot、依此類推。</p>
<p>⚠️ 在這邊有一個小小的重點：遞迴時，資料已經被搬移過了。我們需要保證遞迴下去的時候，資料的分布仍然是「隨機」的：也就是說，對於該子問題，其資料排列在記憶體中任一順序機率均等。（證明可以使用條件機率或組合計數，這邊就略過啦）另一種繞過這個限制的方法，是修改隨機快速排序法，讓它每一次找 pivot 的當下，隨機選取一筆資料作為 pivot，分析起來是一樣的。</p>
<p>於是，利用期望值的可加性，我們可以得到關於期望時間複雜度 \(f(n)\) 的遞迴關係式：</p>
<p>\[
f(n) \le \begin{cases}
O(1) &amp; \text{ if } n \le 1,\\
\Theta(n) + \frac{1}{n} \sum_{p=1}^n (f(p-1) + f(n-p)) &amp; \text{ otherwise.}
\end{cases}
\]</p>
<p>要怎麼證明這個遞迴式解出來會是 \(O(n\log n)\) 呢？最好用的證明工具當然就是數學歸納法囉！</p>
<a class="header" href="#數學歸納法" id="數學歸納法"><h4>數學歸納法</h4></a>
<p>我們想要證明 \(f(n) = cn\log n\)，其中 \(c\) 是某個待定的常數。
假設選定 pivot 以後比較與分組所花費的時間為 \(\le c_0n\)，於是遞迴式會長這樣：</p>
<p>\[
\begin{aligned}
f(n) &amp;\le c_0 n + \frac{1}{n}\sum_{p=1}^n (f(p-1) + f(n-p))\\
&amp;= c_0n + \frac{2}{n}\sum_{i=1}^{n-1} f(i)
\end{aligned}
\]</p>
<p>假設對於所有的 \(n' &lt; n\)，都有 \(f(n') \le cn'\log n'\)。
我們想證明 \(f(n) \le cn\log n\)。而證明的關鍵，是利用後面總和的部分想辦法榨出一個 \(-c_0n\)，消去前面的項。</p>
<p>\[
\begin{aligned}
f(n) &amp;\le c_0n + \frac{2}{n}\sum_{i=1}^{n-1} f(i)\\
&amp;= c_0n + \frac{2}{n}\left(\sum_{i=1}^{\floor{n/2}} f(i) + \sum_{i=\floor{n/2}+1}^{n-1} f(i)\right)\\
&amp;\le c_0n + \frac{2}{n}\left(\sum_{i=1}^{\floor{n/2}} ci\log \frac{n}{2} + \sum_{i=\floor{n/2}+1}^{n-1} ci\log n\right)\\
&amp;= c_0n + \frac{2}{n}\left(\sum_{i=1}^{\floor{n/2}} ci(\log n - 1) + \sum_{i=\floor{n/2}+1}^{n-1} ci\log n\right)\\
&amp;= c_0n + \frac{2}{n}\left(\sum_{i=1}^{n-1} ci\log n - \sum_{i=1}^{\floor{n/2}} ci\right)\\
&amp;= c_0n + c(n-1)\log n - c \frac{2}{n}\frac{\floor{n/2}(\floor{n/2}+1)}{2}\\
&amp;\le c_0n + cn\log n - \frac{c}{4}n + (\frac{c}{4} - c\log n)\\
&amp;\le c_0n + cn\log n - \frac{c}{4}n \\
&amp;= cn\log n + (c_0 - \frac{c}{4})n
\end{aligned}
\]</p>
<p>如果我們選取 \(c \ge 4c_0\)，那麼便能使式子成立，從而命題得證。</p>
<hr />
<p>數學歸納法是很繁瑣的，其他證明方法都不比數學歸納法來得繁瑣。<s>為什麼呢？因為布吉納法索（被拖走）</s></p>
<p>下次我們來看看另外兩個利用期望值可加性和機率方法獲得的證明！</p>
<a class="header" href="#隨機快速排序法-2" id="隨機快速排序法-2"><h1>隨機快速排序法 2</h1></a>
<p>今天我們來看看不使用遞迴方法，直接用機率方法分析隨機快速排序的另一個證明吧。
首先讓我們快速回顧一下定理 13。</p>
<a class="header" href="#定理-13-1" id="定理-13-1"><h2>定理 13</h2></a>
<p>假設 <code>RandomPermute</code> 可以均勻地產生隨機排列，那麼隨機快速排序法的期望時間複雜度為 \(O(n\log n)\)。</p>
<a class="header" href="#在證明開始之前" id="在證明開始之前"><h3>在證明開始之前</h3></a>
<p>讓我們來回憶一下機率名詞。有一種隨機變數，其值只有 0 或 1，代表我們關心的事件出現與否。
而該變數值等於 1 的機率，自然地對應到事件出現的機率。
這樣的隨機變數被稱為<strong>指示隨機變數（Indicator random variable）</strong>。</p>
<p>大家一定還記得，快速排序法的時間複雜度（在一般的實作之下）與資料之間的「兩兩比較次數」呈現正比。
如果我們站在全局視角，關心「任兩個元素被拿來比較」的情形，在快速排序法會不會發生，那麼資料之間的兩兩比較總次數 \(X\)，便可以被拆成任兩筆特定資料被比較的次數 \(X = \sum_{1\le i &lt; j \le n} X_{ij}\)，其中我們用 \(X_{ij}\) 來表示排序完成後，應排在第 \(i\) 位置的資料與第 \(j\) 位置的資料的「是否在演算法執行期間被比較過」的指示隨機變數。</p>
<p>然後！我們就可以利用期望值的可加性，把所有隨機變數拆開來～輕鬆寫出快速排序法的期望時間複雜度：</p>
<p>\(\E[X] = \sum_{1\le i &lt; j \le n} \E[X_{ij}]\text{。}\)</p>
<a class="header" href="#證明二兩兩比較次數的期望值" id="證明二兩兩比較次數的期望值"><h3>證明二：兩兩比較次數的期望值</h3></a>
<p>這麼分析有個好處：我們可以分別計算出 \(\E[X_{ij}]\) 的值。
比起追蹤整個演算法執行時 \(n\) 個物件之間，直接將主力放在某兩個特定的資料上，分析起來可能簡單得多。
在分析開始之前，我們可以試想想看這個值可能會是多少？如果 \(i=1, j=n\)，那麼這兩個數字被直接拿來比較的機率其實相當低。
而這是因為，如果被選中的 pivot 在排序上介於 \(i\) 與 \(j\) 之間的話，顯然 \(i\) 與 \(j\) 會被分派到兩個不同的子問題下去遞迴，自此便無緣相見了。</p>
<p>此外，值得一提的是，在快速排序法進行中，如果有兩筆資料 \(i\) 和 \(j\) 被拿來比較，想必其中一者必定為 pivot。</p>
<p>有了以上的觀察以後，我們現在來說說要證明的敘述：</p>
<a class="header" href="#引理-14" id="引理-14"><h4>引理 14</h4></a>
<p>對於所有的 \(1\le i &lt; j \le n\)，都有 \(\E[X_{ij}] = \frac{2}{j-i+1}\)。</p>
<a class="header" href="#證明-8" id="證明-8"><h4>證明</h4></a>
<p>我們可以偷偷對 \(n\) 用<strong>數學歸納法</strong>來證明這件事情。</p>
<ul>
<li>Base Case: 當 \(n=2\) 的時候，唯一有意義的 \(i, j\) 是 \(i=1, j=2\)。顯然無論如何這兩筆資料都會比較到，因此 \(\E[X_{12}] = 1 = \frac{2}{2-1+1}\) 滿足條件。</li>
<li>Inductive Case: 當 \(n&gt;2\) 的時候，執行一步快速排序法，根據 pivot 的位置 \(p\) 會產生三種情形：
<ul>
<li>pivot 剛好選中 \(i\) 或 \(j\)（事件發生的機率為 \(\frac{2}{n}\)）：此時這兩筆資料必定<strong>會</strong>被比較到。即 \(X_{ij}=1\)。</li>
<li>pivot 滿足 \(p\in [i+1, j-1]\)（事件發生的機率為 \(\frac{j-i-1}{n}\)）：此時兩筆資料必定<strong>不會</strong>被比較。即 \(X_{ij}=0\)。</li>
<li>pivot 滿足 \(p &lt; i\) 或 \(p &gt; j\)（事件發生的機率為 \(1-\frac{j-i+1}{n}\)）：此時 \(i\) 和 \(j\) 會不會被比較到，取決於遞迴呼叫的子問題。在此情形底下，無論子問題的規模有多大，\(i\) 和 \(j\) 總是被分到同一邊。假設這兩筆資料在子問題中的排序位置是 \(i'\) 和 \(j'\)，顯然有 \(j'-i' = j-i\)。
根據歸納假設，此時這兩個數字被比較到的機率恰好等於 \(\frac{2}{j'-i'+1} = \frac{2}{j-i+1}\)。</li>
</ul>
</li>
</ul>
<p>於是乎，我們可將三種情形加起來，得到 \(X_{ij}\) 的期望值：</p>
<p>\[
\begin{aligned}
\E[X_{ij}] &amp;= \frac{2}{n}\cdot 1 + \frac{j-i-1}{n} \cdot 0 + \left( 1 - \frac{j-i+1}{n} \right) \cdot \frac{2}{j-i+1}\\
&amp;= {\color{red}{\frac{2}{n}}} + \frac{2}{j-i+1} {\color{red}{- \frac{j-i+1}{n}\cdot  \frac{2}{j-i+1} }} \\
&amp;= \frac{2}{j-i+1}
\end{aligned}
\]</p>
<p>有了引理 14 以後，我們就可以開心地把所有的 \(i, j\) 對應到的期望值加起來，也就得到了快速排序法的期望時間複雜度了！</p>
<p>\[
\begin{aligned}
\E[X] &amp;= \sum_{1\le i &lt; j \le n} \E[X_{ij}]\\
&amp;= \sum_{1\le i &lt; j \le n} \frac{2}{j-i+1}\\
&amp;= \sum_{\ell=1}^{n-1}\sum_{i=1}^{n-\ell} \frac{2}{\ell+1} &amp; &amp; (\text{變數變換：} j = i+\ell ) \\
&amp;= \sum_{\ell=1}^{n-1} \frac{2(n-\ell)}{\ell+1} \\
&amp;\le 2n\left( \frac{1}{2}+\frac{1}{3} + \cdots + \frac{1}{n} \right)\\
&amp;= O(n\log n)  &amp; &amp; (\text{括弧部分是調和級數所以大約是} \ln n = O(\log n)) \\
\end{aligned}
\]</p>
<hr />
<p>說到底我們還是偷偷用了數學歸納法呀...明天我們來看第三種證明！
保證（希望啦，如果沒想錯的話）沒有數學歸納法！</p>
<a class="header" href="#參考資料-3" id="參考資料-3"><h3>參考資料</h3></a>
<ul>
<li>關於機率論中，更嚴謹的 indicator function / indicator random variable 定義可以參考蔡宗翰的隨筆：https://ch-hsieh.blogspot.com/2012/07/indicator-function.html</li>
</ul>
<a class="header" href="#隨機快速排序法-3" id="隨機快速排序法-3"><h1>隨機快速排序法 3</h1></a>
<p>今天來看看把期望值的可加性發揮到淋漓盡致的一個快速排序法證明。</p>
<a class="header" href="#定理-13-2" id="定理-13-2"><h2>定理 13</h2></a>
<p>假設 <code>RandomPermute</code> 可以均勻地產生隨機排列，那麼隨機快速排序法的期望時間複雜度為 \(O(n\log n)\)。</p>
<a class="header" href="#觀察執行樹" id="觀察執行樹"><h3>觀察：執行樹</h3></a>
<p>如果我們將快速排序法當中，呼叫遞迴解決子問題的所有過程記錄下來，可以描繪出一個樹狀結構。
其中每一個節點都是代表一次的 <code>QuickSort()</code> 呼叫，葉子節點對應到的就是那些 \(n\le 1\) 的邊界條件。</p>
<p><img src="./quicksort3.png" alt="" /></p>
<p>考慮任何一筆資料（如上圖紅色部分），則這筆資料<strong>參與</strong>了許多子問題的計算：包含一開始的整個問題（根節點），直到這筆資料被選為 pivot、或是整個子問題唯一的輸入，才停止參與在演算法的執行之中。
對於每一個子問題，快速排序法對於該次遞迴的執行時間（不計遞迴子問題的時間）其實是線性的。
因此，若我們令隨機變數 \(X_i\) 表示資料 \(i\) 參與的子問題數目，那麼整體的執行時間 \(X\) 將正比於每一筆資料參與的子問題數目總和，即 \(X = X_1+X_2+\cdots + X_n\)。</p>
<a class="header" href="#證明-9" id="證明-9"><h3>證明</h3></a>
<p>既然已將執行時間拆開，利用期望值的可加性，我們只要找出每一個隨機變數 \(X_i\) 的期望值，並且加起來就行了！
而事實上，我們如果著眼在某個特定資料 \(i\)，那麼「隨機選 pivot 並且遞迴下去」的過程，就可以想像成：不斷地隨機選取一個資料，並且把所有跟 \(i\) 不在同一邊的資料丟掉。現在來看看一個決定性的觀察：</p>
<a class="header" href="#引理-15" id="引理-15"><h4>引理 15</h4></a>
<p>假設子問題大小為 \(k\)。那麼，無論我們關心的資料 \(i\) 的排序位置如何，均勻地隨機選取一筆資料作為 pivot 後，至少有 \(\ge 1/3\) 的機率使得包含 \(i\) 的子問題大小變成 \(\le (2/3)k\)。</p>
<a class="header" href="#引理-15-的證明" id="引理-15-的證明"><h4>引理 15 的證明</h4></a>
<p>可以證明，只有在選到排名前 \(1/3\) 的資料、或選到後 \(1/3\) 的資料作為 pivot 的時候，才有可能讓 \(i\) 所在的子問題保有至少 \((2/3)k\) 筆資料。因此其他情形（至少有 \(1/3\) 的機率）都會讓子問題數量縮小 \(2/3\) 倍。</p>
<hr />
<p>有了引理 15 以後，我們可以計算出 \(\E[X_i]\) 的上界了。由於讓子問題縮小 \(2/3\) 倍這件事情，從任何一個資料的角度來看，只會發生至多 \(\log_{3/2} n\) 次。這是因為子問題的大小都是整數，一旦子問題大小小於 1 了遞迴就自然會停下來。
因此，我們可以把 \(X_i\) 拆成許多連續的子問題片段：\(X_i = Z_{i, 1} + Z_{i, 2} + \cdots + Z_{i, {\log_{3/2}} n}\)。其中 \(Z_{i, j}\) 表示從第 \(j-1\) 次把子問題縮小 \(2/3\) 到第 \(j\) 次把子問題縮小 \(2/3\)（或遞迴停止）之間，到底經歷了幾個子問題。
不難發現，因為每一次挑選 pivot 都是隨機地選，而且有至少 \(1/3\) 的機率會讓子問題縮小 \(2/3\)。因此 \(\E[Z_{i, j}] \le 3\)。
於是，我們可以知道 \(\E[X_i] \le 3\log_{3/2} n\)，因此 \(\E[X]\le 3n\log_{3/2} n = O(n\log n)\)。</p>
<hr />
<p>哈！我們利用把整個執行樹展開的觀念，成功避免掉了數學歸納法了。</p>
<a class="header" href="#堆積排序法" id="堆積排序法"><h1>堆積排序法</h1></a>
<p>堆積（Heap）是一種<strong>陣列資料結構</strong>。若我們將 \(n\) 筆資料的堆積以 \(A[1..n]\) 來表示的話，那麼堆積將滿足：對所有 \(x\)，\(A[x] \ge A[2x]\) 以及 \(A[x] \ge A[2x+1]\)（或是指定位置不存在資料）。我們可以按照註標將陣列資料排列成樹狀的結構：</p>
<p><img src="./heapsort1.png" alt="" /></p>
<p>由於每一個節點內所代表的資料都比其子節點來得大，通常我們把這樣的堆積稱為最大堆積（Max-Heap）。相反地，如果我們維護的條件是 \(A[x] \le \min(A[2x], A[2x+1])\)，那麼我們會稱它為最小堆積。</p>
<a class="header" href="#定義-1" id="定義-1"><h3>定義</h3></a>
<p>我們說 \(A[x]\) <strong>滿足堆積性質</strong>，若對於所有樹狀結構中 \(A[x]\) 的子樹內節點 \(A[y]\)，都有 \(A[y] \ge \max(A[2y], A[2y+1])\)。即，如果我們把整個子樹的資料拿出來放在另一個陣列中，那它將是最大堆積。</p>
<p>不難發現，如果 \(A\) 是最大堆積，那麼 \(A[1]\) 的值是整個陣列中所有資料的最大值。</p>
<hr />
<a class="header" href="#heapify---作一個整理成堆積的動作" id="heapify---作一個整理成堆積的動作"><h2>Heapify - 作一個整理成堆積的動作</h2></a>
<p>要怎麼把一個陣列整理成堆積呢？我們只需要一種核心操作「向下交換」：</p>
<pre><code class="language-cpp">void RollDown(data_t *A, const int n, int x) {
  // 邊界條件。
  if (2*x &gt; n) return;
  // 看成樹狀結構時，只有左子樹、沒有右子樹。
  if (2*x+1 &gt; n) {
    if (A[x] &lt; A[2*x]) {
      swap(A[x], A[2*x]);
    }
    return;
  }
  // 若兩邊子樹都存在，則挑大的那邊交換下去，並且遞迴處理。
  if (A[x] &lt; max(A[2*x], A[2*x+1])) {
    int larger = (A[2*x] &gt; A[2*x+1] ? 2*x : 2*x+1);
    swap(A[x], A[larger]);
    RollDown(A, n, larger);
  }
}
</code></pre>
<a class="header" href="#引理-16" id="引理-16"><h3>引理 16</h3></a>
<p>若 \(A[2x]\) 與 \(A[2x+1]\) 分別滿足堆積性質，那麼呼叫 <code>RollDown(x)</code> 以後，\(A[x]\) 將滿足堆積性質。</p>
<a class="header" href="#證明-10" id="證明-10"><h3>證明</h3></a>
<p>可以利用數學歸納法來證明：假設對於所有的 \(x' &gt; x\) 都已成立。
那麼對於現在這個 \(x\) 來說，在呼叫 <code>RollDown(x)</code> 以後，首先我們能保證 \(A[x] \ge \max(A[2x], A[2x+1])\)。
接下來，我們檢視哪些元素被動過了：如果剛才交換的是 \(A[x]\) 與 \(A[2x]\)，那麼另一邊（\(A[2x+1]\)）整棵子樹是不會變的。
此時，根據歸納假設 \(2x &gt; x\)，顯然在呼叫下一層遞迴之前，\(A[4x]\) 與 \(A[4x+1]\) 的資料都沒有變動過，因此它們滿足堆積性質。在呼叫遞迴之後，\(A[2x]\) 便滿足了堆積性質，從而得證 \(A[x]\) 也滿足堆積性質。根據對稱性，如果剛才交換的是 \(A[x]\) 與 \(A[2x+1]\)，也會保證遞迴執行完畢後，\(A[x]\) 也滿足堆積性質。</p>
<hr />
<p>有了引理 16 以後，我們就可以輕鬆把整個陣列變成堆積了：</p>
<pre><code class="language-cpp">void Heapify(data_t *A, const int n) {
  // 注意這邊有別於一般陣列：傳入之陣列範圍是 A[1]...A[n]。
  for (int x = n; x &gt;= 1; --x) {
    RollDown(A, n, x);
  }
}
</code></pre>
<p>正確性可以利用引理 16 輕鬆證得，比較有趣的是時間複雜度的分析。</p>
<a class="header" href="#引理-17" id="引理-17"><h3>引理 17</h3></a>
<p><code>Heapify()</code> 的時間複雜度為 \(O(n)\)。</p>
<a class="header" href="#證明-11" id="證明-11"><h3>證明</h3></a>
<p>對於每一個註標 \(x\)，呼叫 <code>RollDown(x)</code> 時，每一次遞迴呼叫時至少將參數乘以 2，直到超過 \(n\) 為止。因此單次呼叫 <code>RollDown(x)</code> 的執行時間不超過 \(\ceil{\log\frac{n}{x}}\)。把它全部加起來，會得到</p>
<p>\[
\begin{aligned}
T(n) \le \sum_{x=1}^n \ceil{\log \frac{n}{x}} &amp;\le \sum_{x=1}^n \left(1 + \log \frac{n}{x}\right)\\
&amp;\le n + \sum_{x=1}^n \log \frac{n}{x}
\end{aligned}
\]</p>
<p>令 \(S(n) = \sum_{x=1}^n \log\frac{n}{x}\)。推敲一陣可知
\[
S(n) = {\color{purple}{\sum_{x=1}^{n/2} \log\frac{n/2}{x}}} + {\color{green}{\frac{n}{2} + \sum_{x=n/2+1}^n \log\frac{n}{x}}}
\le {\color{purple}{S(n/2)}} + {\color{green}{n}}
\]</p>
<p>於是一路推敲下去可以用數學歸納法證得 \(S(n)\le 2n\)。所以 Heapify 整體時間複雜度為 \(T(n) \le n + S(n) \le 3n = O(n)\)。</p>
<hr />
<a class="header" href="#堆積排序" id="堆積排序"><h2>堆積排序</h2></a>
<p>有了堆積以後，就可以進行堆積排序囉。注意到，一個最大堆積其最大值總是出現在 \(A[1]\) 的位置。因此，我們可以重複地將最大值放到堆積的末端（第一次交換 \(A[1]\) 與 \(A[n]\)、第二次交換 \(A[1]\) 與 \(A[n-1]\)、依此類推）最終就可以得到一個由小到大排好順序的序列啦！</p>
<pre><code class="language-cpp">void HeapSort(data_t *A, int n) {
  // 注意這邊有別於一般陣列：傳入之陣列範圍是 A[1]...A[n]。
  Heapify(A, n);
  while (n &gt; 1) {
    swap(A[1], A[n--]);   // 把最大值放到最後一格，並縮減堆積範圍。
    RollDown(A, n, 1);    // 整理完後又是一個最大堆積！
  }
}
</code></pre>
<p>這個方法的時間複雜度是 Heapify 的時間、加上執行 \(n\) 次 <code>RollDown()</code> 的時間。
前者耗費 \(O(n)\)、後者耗費 \(\sum_{i=1}^n \log i = \Theta(n\log n)\)。
因此總執行時間為 \(O(n\log n)\)。</p>
<a class="header" href="#備註" id="備註"><h3>備註</h3></a>
<p>這個排序方法所需要的額外空間是 \(O(1)\)，因此堆積排序法是一種原地演算法。</p>
<a class="header" href="#比較排序下界" id="比較排序下界"><h1>比較排序下界</h1></a>
<p>到目前為止我們看過的排序方法，都是只要支援「能夠兩兩互相比較」，就可以由小到大排好序的演算法。
其中幾個排序演算法如：合併排序法、隨機排序法、堆積排序法等等，都能夠達到 \(O(n\log n)\) 的時間複雜度，幾乎線性。
但終究還是與顯然下界（至少要看過所有輸入的資料才能排序，因此任何能夠將資料正確排序的演算法都必須花費 \(\Omega(n)\) 的時間）有著隔閡。下面這個定理告訴我們，如果只計算比較次數的話，我們能得到更大的下界。</p>
<a class="header" href="#定理-18" id="定理-18"><h3>定理 18</h3></a>
<p>任何一個確定性演算法，如果存取輸入資料時<strong>僅</strong>能夠進行「兩兩互相比較」。那麼排序 \(n\) 筆資料，最壞情形下至少要進行 \(\ceil{\log n!} = \Omega(n\log n)\) 次比較。</p>
<a class="header" href="#證明-12" id="證明-12"><h3>證明</h3></a>
<p>對於輸入的 \(n\) 筆資料，排序完畢後的輸出，相對於輸入總是一個排列（Permutation）。由於 \(n\) 筆資料總共有 \(n!\) 種不同的排列情形，每問一個問題「是否 \(A[i] &gt; A[j]\)？」，我們可以把仍滿足先前所有提問的排列找出來，形成集合 \(\mathcal{S}\)，然後分成兩組：滿足當前這個問題（\(A[i] &gt; A[j]\)）的所有排列 \(\mathcal{S}_{yes}\)、或者是不滿足當前問題（\(A[i] \le A[j]\)）的所有排列 \(\mathcal{S}_{no}\)。這兩個排列集合是不相交的，換句話說，如果我是邪惡的餵輸入的人，我可以設計一組輸入，讓滿足所有到目前為止的排列們，恰好就是 \(|\mathcal{S}_{yes}\mid \) 或 \(|\mathcal{S}_{no}\mid \) 之間比較大的那一個。
換句話說，在運氣最好的情況下，你的演算法也只能夠剔除一半的排列數。</p>
<p>為了讓演算法完全正確，任何確定性的演算法，在 \(|\mathcal{S}\mid  &gt; 1\) 的任何時刻，演算法都不能輸出答案。
否則的話，總是存在另兩種不同的輸入（\(1\) 到 \(n\) 的排列），經歷過一連串比較得到一模一樣的答案以後，演算法就停下來而且輸出了其中一個作為答案。此時，另一個輸入再餵進同樣的演算法，就會錯掉了。
至少要經過幾次比較，才能讓滿足條件的排列集合 \(\mathcal{S}\) 變成唯一的排列呢？由於每一個問題只能讓排列數降低成一半，至少得經過 \(\ceil{\log n!}\) 次比較才行。</p>
<p>於是我們就得到 \(\Omega(n\log n)\) 的比較排序下界啦！</p>
<blockquote>
<p>這個證明是少數幾個能夠簡單推論出的非顯然下界，我們應該要多多珍惜。
一般來說，要證明時間複雜度下界，通常可以從資訊理論（Information Theory）的角度，加上決策樹來下手。</p>
</blockquote>
<hr />
<p>我們可以把這套方法，應用在其他類似的問題中。比方說，假設我們有兩個已經排好序的數列 \(A[1..n]\)、以及 \(B[1..m]\)。
合併排序法的「合併」步驟，需要花費 \(O(n+m)\) 次比較。這個方法是最優的嗎？我們可以透過類似的分析，得到有趣的結論：</p>
<a class="header" href="#定理-19" id="定理-19"><h3>定理 19</h3></a>
<p>合併兩個已排序的序列 \(A[1..n]\) 以及 \(B[1..m]\)。假設 \(n\le m\)，那麼，任何確定性演算法，都至少需要 \(\Omega(n\log \frac{m}{n})\) 次比較才行。</p>
<a class="header" href="#證明-13" id="證明-13"><h3>證明</h3></a>
<p>合併完畢的結果，可以一一對應到把 \(n+m\) 個連續的格子進行黑白染色，使得恰好有 \(n\) 個黑色格子、有 \(m\) 個白色格子。其中黑色格子就依序對應了 \(A\) 陣列的內容、白色格子就依序對應了 \(B\) 陣列的內容。
由於每一次比較也只能剔除一半的黑白染色方法，因此任何一個確定性演算法，在最壞情形下至少得進行 \(\ceil{\log {m+n\choose n}}\) 次比較。</p>
<p>於是，在 \(n\le m\) 的時候，我們有
\(\log {m+n\choose n}\ge \log \frac{m^n}{n!} \approx n\log m - n\log n = n\log \frac{m}{n}\)，得證。</p>
<a class="header" href="#這個證明告訴我們什麼" id="這個證明告訴我們什麼"><h3>這個證明告訴我們什麼？</h3></a>
<p>如果 \(n\) 與 \(m\) 比例懸殊（比方說極端情形 \(n=1\)），那麼，比方說，我們可以用「二分搜尋法」，對每一個 \(A\) 中的元素，找到適合插入在陣列 \(B\) 裡面的位置。有了這些位置以後，就可以在「不查看大部分資料內容」的情形下，合併兩個陣列了！
雖然合併陣列的時間複雜度顯然得花 \(O(n+m)\) 的時間，但這些時間是大多花費在資料的搬移上，我們降低的是「進行決策」所花費的時間複雜度（從另一個角度來看，也可以說是減少可能產生的程序分支 branching！）</p>
<hr />
<p>有沒有更快的排序演算法呢？這個取決於你的計算模型！
我們現在使用的電腦，顯然比「兩兩互相比較」模型還要強得多。
而事實上，是比較接近「隨機存取模型」的。因此，在不同計算模型的條件下，我們被允許對資料有更多種類的操作，或許存在著時間複雜度更低的演算法。</p>
<a class="header" href="#推薦閱讀-2" id="推薦閱讀-2"><h3>推薦閱讀</h3></a>
<ul>
<li>德州農工(TAMU)投影片：http://faculty.cs.tamu.edu/klappi/csce411-f17/csce411-set3.pdf</li>
</ul>
<!-- * 偷推林軒田教授的資訊理論課程：https://www.csie.ntu.edu.tw/~htlin/course/iit19fall/ -->
<a class="header" href="#最少比較排序" id="最少比較排序"><h1>最少比較排序</h1></a>
<p>前一篇我們討論了比較排序的下界。對於所有的 \(n\)，基於比較的排序方法至少要花 \(\ceil{\log n!}\) 次比較。
而根據合併排序法、或快速排序法等，我們也知道要完成排序至多只需要 \(O(n\log n)\) 次比較。
Lower bound 與 Upper bound 完美地合起來了不是嗎！
俗話說得好，魔鬼藏在細節裡，常數藏在 big-O 裡面。
對於六〇年代的電腦科學家們，不把常數寫清楚是會對自己過意不去的。
換句話說，若先不考慮把演算法實作出來後真正的時間複雜度，我們只關心「比較次數」的話，是否<strong>總是</strong>存在一種排序的演算法，在最壞情形下只需要恰好 \(\ceil{\log n!}\) 次比較就能夠排好序呢？</p>
<p>答案是否定的。但是在描述最小反例之前，我們不妨先想想看，究竟可以設計出怎麼樣的演算法，其所需的比較次數與 \(\ceil{\log n!}\) 足夠接近：</p>
<a class="header" href="#引理-20" id="引理-20"><h2>引理 20</h2></a>
<p>存在一種比較排序法，使得排好 \(n\) 筆資料至多需要 \(\ceil{\log 2} + \ceil{\log 3} + \cdots + \ceil{\log n}\) 次比較。</p>
<a class="header" href="#引理-20-的證明" id="引理-20-的證明"><h3>引理 20 的證明</h3></a>
<p>上面這個式子給我們很大的提示：我們只要稍微修改一下<strong>插入排序法</strong>，每一次加入一個數字。但是在加入的時候，我們不從當前序列末端一路比較過來；相反地，我們使用定理 19 提到的<strong>二分搜尋法</strong>，把資料插入目前排好序的序列就可以了。
我們不需要擔心資料搬移所花費的時間，反正資料的搬移在這個比較次數至上的計算模型裡面是免費的！</p>
<hr />
<p>看來細節不只藏在 big-O 裡面，還藏在天花板裡面呢。</p>
<p>上面這兩個函數到底差多少？令 \(A(n) = \ceil{\log n!}\)、令 \(B(n) = \sum_{i=1}^n \ceil{\log i}\)。我們可以簡單寫張表，列出前面幾項數值：</p>
<table><thead><tr><th align="center"> \(n\) </th><th align="center"> 1 </th><th align="center"> 2 </th><th align="center"> 3 </th><th align="center"> 4 </th><th align="center"> 5 </th><th align="center"> 6 </th><th align="center"> 7 </th><th align="center"> 8 </th><th align="center"> 9 </th><th align="center"> 10 </th><th> 11 </th><th> 12 </th></tr></thead><tbody>
<tr><td align="center"> \(A(n)\) </td><td align="center"> 0 </td><td align="center"> 1 </td><td align="center"> 3 </td><td align="center"> 5 </td><td align="center"> 7 </td><td align="center"> 10 </td><td align="center"> 13 </td><td align="center"> 16 </td><td align="center"> 19 </td><td align="center"> 22 </td><td> 26 </td><td> 29 </td></tr>
<tr><td align="center"> \(B(n)\) </td><td align="center"> 0 </td><td align="center"> 1 </td><td align="center"> 3 </td><td align="center"> 5 </td><td align="center"> 8 </td><td align="center"> 11 </td><td align="center"> 14 </td><td align="center"> 17 </td><td align="center"> 21 </td><td align="center"> 25 </td><td> 29 </td><td> 33 </td></tr>
</tbody></table>
<p>在 \(n=5\) 的時候數字就不一樣了！這不禁讓我們思考：要排好 5 個數字，至少需要進行幾次比較呢？如果是 7 次，那麼代表前述之「二分插入法」不能達到最少比較次數；反之，如果是 8 次，代表基於資訊理論方法得到的下界不是最緊的。無論是哪一種結論好像都讓我們多理解了什麼，對吧～</p>
<a class="header" href="#引理-21" id="引理-21"><h2>引理 21</h2></a>
<p>存在一個比較排序法，使得正確排序 5 筆資料，在最壞情形下僅需要 7 次比較。這結果真的是令人五筆振奮啊（這裡沒有梗）。</p>
<a class="header" href="#引理-21-的證明" id="引理-21-的證明"><h3>引理 21 的證明</h3></a>
<p>證明實在難以用言語形容，直接畫圖比較快。這邊我們假設輸入為 \(A[1..5]\)。</p>
<p><img src="./minimum-comparison-sort.png" alt="" /></p>
<p>你看看，無論輸入長什麼樣子，這個決策樹總可以在至多 7 次以內順利排序完畢。而這邊使用到的是二分插入法的概念，只是用了比較取巧的比較順序。西元 1959 年 Ford-Johnson 把這個想法延伸，設計出了一個叫做<strong>合併插入排序法 Merge Insertion Sort</strong> 的排序演算法（其中 Ford 就是 Ford-Fulkerson 演算法裡面的 Ford、Johnson 就是 Johnson 演算法裡面的那位、好吧我不知道我在講什麼。）</p>
<p>如果我們把 FJ 演算法需要的比較次數的前幾項寫出來：</p>
<table><thead><tr><th> \(n\) </th><th> 1 </th><th> 2 </th><th> 3 </th><th> 4 </th><th> 5 </th><th> 6 </th><th> 7 </th><th> 8 </th><th> 9 </th><th> 10 </th><th> 11 </th><th> 12 </th></tr></thead><tbody>
<tr><td align="center"> \(A(n)\) </td><td align="center"> 0 </td><td align="center"> 1 </td><td align="center"> 3 </td><td align="center"> 5 </td><td align="center"> 7 </td><td align="center"> 10 </td><td align="center"> 13 </td><td align="center"> 16 </td><td align="center"> 19 </td><td align="center"> 22 </td><td> 26 </td><td> 29 </td></tr>
<tr><td align="center"> \(FJ(n)\) </td><td align="center"> 0 </td><td align="center"> 1 </td><td align="center"> 3 </td><td align="center"> 5 </td><td align="center"> 7 </td><td align="center"> 10 </td><td align="center"> 13 </td><td align="center"> 16 </td><td align="center"> 19 </td><td align="center"> 22 </td><td> 26 </td><td> 30 </td></tr>
</tbody></table>
<p>從 \(n=2...11\) 全部都是最佳解了！真正難纏的部分是 \(n=12\)，直到 1965 年 Mark Wells<sup class="footnote-reference"><a href="#1">1</a></sup> 率先撰寫程式列舉所有排序結構，證明了 \(S(12) = 30\)（有興趣的朋友可以參考 Knuth 的 TAOCP，第三卷）。這個結果證明了資訊理論下界不等於最少排序次數。</p>
<p>Ford-Johnson 的合併插入排序法是否真的是最優的呢？可惜的是 1977 年 Manacher<sup class="footnote-reference"><a href="#2">2</a></sup> 否定了這件事情：他證明了存在無窮多個 \(n\)，使得最少排序次數比 \(FJ(n)\) 嚴格來得小。目前已知最小的反例是在 \(n=189\)。</p>
<hr />
<p>如同四色定理一樣，找出最少比較排序的次數，可以藉由電腦輔助而完成證明。很酷吧！</p>
<a class="header" href="#推薦閱讀-3" id="推薦閱讀-3"><h3>推薦閱讀</h3></a>
<ul>
<li>高德納教授（Knuth）的《The Art Of Computer Programming》第 5.3.1 節。</li>
<li>《最少排序問題中 \(S(15)\) 與 \(S(19)\) 的解決》：http://fcst.ceaj.org/EN/abstract/abstract47.shtml</li>
<li>13, 14, 22 個元素排序：https://link.springer.com/content/pdf/10.1007%2Fs00453-004-1100-7.pdf</li>
<li>\(FJ(n)\) 在 \(n&lt;47\) 以前都是好的：https://www.sciencedirect.com/science/article/pii/S0020019006002742</li>
</ul>
<div class="footnote-definition" id="1"><sup class="footnote-definition-label">1</sup>
<p>Mark B. Wells, <strong>Applications of a language for computing in combinatorics</strong>, IFIP 1965.</p>
</div>
<div class="footnote-definition" id="2"><sup class="footnote-definition-label">2</sup>
<p>Glenn K. Manacher, <strong>The Ford-Johnson Sorting Algorithm Is Not Optimal</strong>, 1979. https://dl.acm.org/doi/pdf/10.1145/322139.322145</p>
</div>
<a class="header" href="#合併插入排序" id="合併插入排序"><h1>合併插入排序</h1></a>
<p>Lester Ford, Jr. 以及 Selmer Johnson 把 <a href="https://www.uidaho.edu/engr/departments/ece/our-people/emeriti/howard-demuth">Howard B. Demuth</a> 的 1957 年博士論文裡面提到的 5 筆資料排序方法進行推廣，最終獲得一個用謹慎的方法試圖減少比較次數的排序方法──<strong>合併插入排序 Merge Insertion Sort</strong>。
這個排序法的名字是高德納教授（Donald Knuth）在他撰寫的《The Art of Computer Programming》裡面取的。
附帶一提，最近 MIT 教授 Lex Fridman 釋出了一段與 Knuth 的訪談<sup class="footnote-reference"><a href="#1">1</a></sup>：https://www.youtube.com/watch?v=2BdBfsXbST8 大家有興趣可以聽聽看。</p>
<a class="header" href="#如何避免大量的天花板" id="如何避免大量的天花板"><h2>如何避免大量的天花板</h2></a>
<p>演算法的核心概念是這樣的：如果看到一個長度為 \(2^k-1\) 元素的序列，那麼此時將一個新的元素插入其中，是最不會浪費「資訊」的。因為資訊理論下界 \(\ceil{\log_2 ((2^k-1)+1)}\) 無論有沒有天花板，其數值都是一樣的，得花費恰巧 \(k\) 次才能夠找出新元素的落點。</p>
<p>如果長度不到 \(2^k-1\) 的序列怎麼辦？盡量讓這件事情不要發生就好了！
乍看之下很困難啊──但是 Ford-Johnson <strong>盡量</strong>做到了，最重要的想法可以透過下面這張圖來傳達：</p>
<p><img src="./merge-insertion-sort1.png" alt="" /></p>
<p>假設我們能夠經過若干次比較之後，把所有元素的大小關係描述成上面這張圖。其中 \(x\to y\) 代表經過比較以後，我們得出了 \(x &lt; y\) 這項結論。
上圖黃色框線內共有 7 個元素，若我們想將紅色元素插入上面長鏈中，此時恰好達到資訊理論下界的 \(\log_2(7+1) = 3\) 次。因此可以得到結論是：此時優先將紅色元素插入長鏈、再將左邊的藍色元素插入長鏈。其需要的比較次數至多是 \(\log_2(7+1) + \log_2(7+1) = 6\) 次。相反地，若我們先插入藍色元素，再插入紅色元素，在最壞情形下我們得花費 \(\log_2(7+1) + \ceil{\log_2(8+1)} = 7\) 次比較才能達到效果。</p>
<p>行文至此，不難發現，如果我們有辦法把輸入資料的大小關係，表達成上圖這種牙刷形狀，再依照最不浪費比較次數的方式進行二分插入，說不定可以得到較佳（比較次數較小）的排序演算法。</p>
<a class="header" href="#合併插入排序的第一步" id="合併插入排序的第一步"><h2>合併插入排序的第一步</h2></a>
<p>要怎麼生出牙刷呢？
首先，在分而治之的部分，我們先將資料隨意地兩兩分成一組，並且花費 \(\floor{n/2}\) 次比較。
如果有多出來的元素，就先放在旁邊吧。
接下來，我們可以遞迴針對比較大的那些 \(\floor{n/2}\) 元素進行排序，就可以把它們接成一長串了！
最後是刷毛整理的部分：我們將刷毛由左至右（資料可以命名為 \(b_1, b_2, \ldots, b_{\ceil{n/2}}\)）分成若干組，而每一組的數量都會滿足：把這組資料由右至左依序進行二分插入法，都是最不浪費資訊的。</p>
<p>我們現在來引用 Donald Knuth 《The Art of Computer Programming》裡面提到的分析技巧<sup class="footnote-reference"><a href="#2">2</a></sup>：</p>
<p><img src="./merge-insertion-sort2.png" alt="" /></p>
<p>這些組別分起來，會長得像這樣：\(\{b_1\}\)、\(\{b_2, b_3\}\)、\(\{b_4, b_5\}\)、\(\{b_6, b_7, b_8, b_9, b_{10}, b_{11}\}\)…。我們可以令 \(t_k\) 代表最大的註標使得把跟它分派到同一組的所有元素，依照反過來的順序依序二分插入序列時，最壞情形下每次都恰好需要 \(k\) 次比較。即，跟 \(t_0\) 同一組的資料插入序列恰好需要 0 次比較、跟 \(t_2\) 同一組的資料插入序列需要 2 次比較、依此類推。</p>
<p>要怎麼找出 \(t_k\) 的值呢？注意到，當 \(b_{t_k}\) 的筆資料被插入序列的時候，所有註標不超過 \(t_{k-1}\) 的資料已經被加入序列了。此外，所有註標等於 \(t_{k-1}+1, \ldots, t_k\) 的元素都還沒有被加入序列。也就是說，此時序列恰恰好有 \(2t_{k-1}+(t_k-t_{k-1}-1) = t_{k-1}+t_k-1\) 筆資料！（這個 \(-1\) 是因為最後一個註標 \(b_{t_k}\) 頭頂上那筆資料不需要被考慮進去，它的資料比所有人都來得大）為了讓資訊不浪費，我們希望 \(t_{k-1}+t_k-1 = 2^k-1\)。因此得到
\[
t_{k-1} + t_k = 2^k\text{。}
\]
經過一番寒徹骨，不是，經過一翻兩瞪眼以後，不是，總之，經過一直翻一直翻，我們可以得出一個神奇的結論：
\[
\begin{aligned}
t_k &amp;= 2^k - 2^{k-1} + 2^{k-2} - \cdots + (-1)^k2^0\\
&amp;= (2^{k+1}+(-1)^k)/3.
\end{aligned}
\]</p>
<p>有了這個演算法以後，我們就可以利用遞迴方法來分析，這個合併插入排序需要的比較總次數了。我們令 \(F(n)\) 表示對 \(n\) 筆資料進行 Merge-Insertion Sort 最壞情形下所需要的比較次數。
那麼可以得到遞迴關係：</p>
<p>\[
F(n) = \floor{n/2} + F(\floor{n/2}) + G(\ceil{n/2})
\]</p>
<p>其中，\(G(\ceil{n/2})\) 表示著當牙刷刷毛有 \(\ceil{n/2}\) 根的時候（分別是 \(b_1, b_2, \ldots, b_{\ceil{n/2}}\)），要合併成一條單鏈需要的比較次數。藉由 \(\{t_i\}\) 的表示法，若 \(t_{k-1} &lt; \ceil{n/2}\le t_k\)，我們可以把 \(G(\ceil{n/2})\) 寫成</p>
<p>\[
\begin{aligned}
G(\ceil{n/2}) &amp;= \sum_{i=1}^{k-1} i (t_i-t_{i-1}) + k(\ceil{n/2} - t_{k-1}) \\
&amp;= k\ceil{n/2} - (t_0 + t_1 + \cdots + t_{k-1})
\end{aligned}
\]</p>
<p>令 \(w_k = t_0+t_1+\cdots + t_{k-1} = \floor{2^{k+1}/3}\)。
現在來證明今天最重要的一個結論：</p>
<a class="header" href="#引理-22" id="引理-22"><h3>引理 22</h3></a>
<p>\( F(n) - F(n-1) = k \) 若且唯若 \(w_k &lt; n \le w_{k+1}\)。</p>
<a class="header" href="#證明-14" id="證明-14"><h3>證明</h3></a>
<p>我們可以用數學歸納法。Base Case 很顯然，所以就不寫了。Inductive Case 的部分可以利用 \(n\) 的奇偶性分別討論：如果 \(n\) 是偶數，那麼 \(F(n)-F(n-1) = 1+F(n/2)-F(n/2-1)\)，後半部的數值等於 \(k-1\) 若且唯若 \(w_{k-1} &lt; \floor{n/2} \le w_{k}\)。而由 \(w_k\) 之定義可知</p>
<p>\[
\begin{aligned}
&amp;&amp; w_{k-1} &amp; &lt; &amp; n/2 &amp; \le   w_{k}  &amp; \text{（} n \text{ 是偶數。）}\\
&amp;\Longleftrightarrow &amp; \floor{2^{k}/3} &amp; &lt; &amp; n/2 &amp; \le  \floor{2^{k+1}/3} \\
&amp;\Longleftrightarrow &amp; 2\floor{2^{k}/3} &amp; &lt; &amp; n &amp; \le  2\floor{2^{k+1}/3} \\
&amp;\Longleftrightarrow &amp; 2\floor{2^{k}/3}+1 &amp; &lt; &amp; n &amp; \le  2\floor{2^{k+1}/3} &amp; \text{（} n \text{ 是偶數，這很重要。）} \\
&amp;\Longrightarrow &amp; \floor{2^{k+1}/3} &amp; &lt; &amp; n &amp; \le  \floor{2^{k+2}/3} \\
\end{aligned}
\]</p>
<p>於是 \(n\) 是偶數的時候結論成立。第二種情形，當 \(n\) 是奇數的時候，我們可以依樣畫葫蘆：</p>
<p>\[
F(n)-F(n-1) = G(\ceil{n/2})-G(\ceil{(n-1)/2})
\]</p>
<p>然後這個值是 \(k\) 若且唯若 \(t_{k-1} &lt; \ceil{n/2} \le t_k\)，然後這個等價於 \(w_k &lt; n\le w_{k+1}\)，得證。</p>
<hr />
<p>有了引理 22 以後，我們試圖找出 \(k\) 與 \(n\) 之間的關係。因為 \(w_k &lt; n \le w_{k+1}\)，所以 \(k\) 可以寫成 \(\ceil{\log_2 \frac{3}{4}n}\)。於是得到很酷的結論：</p>
<p>\[
F(n) = \sum_{i=1}^n \ceil{\log_2 \frac{3}{4}i} \approx n\log_2 n - 1.415n + O(\log_2 n)
\]</p>
<p>還記得二分插入法的上界、以及資訊理論下界嗎？我們把它們同步列出來：</p>
<p>\[
A(n) = \ceil{\log n!}  \approx n\log_2 n - 1.443 n + O(\log_2 n)
\]</p>
<p>\[
B(n) = \sum_{i=1}^n \ceil{\log i} \approx n\log_2 n - 0.915 n + O(\log_2 n)
\]</p>
<p>不難發現 \(F(n)\) 比起二分插入法得到的比較次數 \(B(n)\)，更接近資訊理論下界 \(A(n)\) 了呢！</p>
<a class="header" href="#結論" id="結論"><h3>結論</h3></a>
<p>後話就是，在 1979 年 Manacher<sup class="footnote-reference"><a href="#3">3</a></sup> 用了混合方法，把 Hwang-Lin<sup class="footnote-reference"><a href="#5">4</a></sup> 的兩序列合併演算法考慮進去並且在某些情形下改良，同時改進了 Ford-Johnson 演算法，並且證明了<strong>存在無窮多個</strong> \(n\)，其最少比較排序的比較次數比 \(F(n)\) 嚴格來得小，該方法贏過 FJ 的最小值是 \(n=189\)。然後到了 1985 年 Bui 和 Thanh<sup class="footnote-reference"><a href="#4">5</a></sup> 再次修改了 Manacher 演算法，得出在絕大多數的 \(n\) 時，FJ 演算法不是最優的。而且也得出了最小反例出現在 \(n=47\)（到 2007 年為止，Peczarski<sup class="footnote-reference"><a href="#6">6</a></sup> 聲稱 FJ 演算法在 \(n&lt;47\) 的時候是最佳解，Peczarski 證明了利用某一類型的分而治之演算法無法在 \(n\le 46\) 的時候贏過 FJ 演算法。）</p>
<p>然後 \(F(n)\) 其實有封閉形式（Closed Form，俗稱公式解）：
\[
F(n) = n\ceil{\log_2 \frac{3}{4} n} + \floor{\frac13 2^{\floor{\log_2 6n}}} + \floor{\frac12\log_2 6n}\text{。}
\]</p>
<p>最後，大家要多多刷牙喔 ^_&lt;。</p>
<hr />
<p>合併插入排序法中間的分析，利用到了「如何從排序到一半的東西，利用盡量少比較次數完成排序」的特性。我們是否可以把這個概念推廣一下呢？排序到一半的東西，可以被表示成一個叫做 <strong>偏序集（Partial Ordered Set）</strong> 的東西。我們能否從偏序集獲得一些排序知識呢？</p>
<a class="header" href="#推薦閱讀-4" id="推薦閱讀-4"><h3>推薦閱讀</h3></a>
<ul>
<li>維基百科：https://en.wikipedia.org/wiki/Merge-insertion_sort</li>
<li>Ford-Johnson 演算法當年的論文：https://www.jstor.org/stable/pdf/2308750.pdf</li>
</ul>
<div class="footnote-definition" id="1"><sup class="footnote-definition-label">1</sup>
<p>感謝 a127 的告知與推薦！</p>
</div>
<div class="footnote-definition" id="2"><sup class="footnote-definition-label">2</sup>
<p>Donald Knuth, <em>The Art of Computer Programming</em>, Volumn 3, Page 183-187.</p>
</div>
<div class="footnote-definition" id="3"><sup class="footnote-definition-label">3</sup>
<p>Glenn K. Manacher, <em>The Ford-Johnson Sorting Algorithm Is Not Optimal</em>, 1979. https://dl.acm.org/doi/pdf/10.1145/322139.322145</p>
</div>
<div class="footnote-definition" id="4"><sup class="footnote-definition-label">5</sup>
<p>T. D. Bui and Mai Thanh, <em>Significant improvements to the Ford-Johnson algorithm for sorting</em>, BIT 1985. https://link.springer.com/article/10.1007/BF01934989</p>
</div>
<div class="footnote-definition" id="5"><sup class="footnote-definition-label">4</sup>
<p>交大黃光明教授與 S. Lin（我查不到…）當年在貝爾實驗室發表的論文：F. K. Hwang and S. Lin, <em>A Simple Algorithm for Merging Two Disjoint Linearly Ordered Sets</em>, SIAM Journal of Computation, 1972. https://epubs.siam.org/doi/abs/10.1137/0201004</p>
</div>
<div class="footnote-definition" id="6"><sup class="footnote-definition-label">6</sup>
<p>Marcin Peczarski, <em>The Ford–Johnson algorithm still unbeaten for less than 47 elements</em>, IPL 2007 February. https://www.sciencedirect.com/science/article/pii/S0020019006002742</p>
</div>
<a class="header" href="#偏序集排序一" id="偏序集排序一"><h1>偏序集排序（一）</h1></a>
<p>在比較排序的過程中，我們不斷地詢問某兩筆資料之間的大小關係。
到目前為止，我們知道，從資訊理論的角度來看，若採用兩兩資料比較的方式逐漸取得訊息，
從一個「完全不知道訊息之間大小關係」，與「知曉任兩筆資料之間的大小關係」，
至少要經過 \(\ceil{\log n!}\) 次詢問。</p>
<p>如果我們想要證明更嚴謹的排序次數下界（比方說最小反例 \(S(12) &gt; 29\)）。我們必須要考慮所有的排序方法：證明任何一種排序方法都無法在 29 次比較之內得到 12 筆資料的完整排序。
但是，演算法顯然有無窮多種（想像一下：若我們將演算法以 C 語言實作，雖然都是排序 12 筆資料，但是有無窮多種寫法可以正確地排序這 12 種資料。）
在只關心比較次數的情況下，我們是否有辦法系統性地描述「訊息逐漸揭露」的過程呢？</p>
<p>要理解這個問題，讓我們從定義<strong>偏序集（Partially Ordered Sets）</strong> 開始吧！</p>
<!-- 今天來簡單介紹**偏序集（Partially Ordered Sets）**、**線性延伸（Linear Extension）**、以及由 Frank Hwang 與 Shen Lin 定義的**排序效率（Efficiency）**。
以圖論語言來說明的話，我們今天想介紹的是**有向無環圖（Directed Acyclic Graph）**、**拓撲順序（Topological Order）**、以及基於這兩個觀念定義出來的排序效率。 -->
<a class="header" href="#偏序集-poset" id="偏序集-poset"><h2>偏序集 Poset</h2></a>
<p>首先，我們不妨假設任何兩筆輸入的資料都不相同，而且所有資料存在一個全序排列（Total Order），也就是說，隨意選取任何兩筆資料 \(x\) 和 \(y\)，必定有 \(x &lt; y\) 或者 \(x &gt; y\)。</p>
<p>假設某個演算法今天對於 \(n\) 筆資料，抓了其中兩筆資料 \(x_1, x_2\) 來比較大小，獲得了 \(x_1 &lt; x_2\) 這樣的資訊。於是呢，我們可以從 \(x_1\) 到 \(x_2\) 畫一個箭頭 \(x_1\to x_2\)。這些資料與資料之間的關聯，形成一個偏序集 Partially Ordered Set，通常我們會簡稱 Poset，以 \((P, &lt;)\) 表示。
這個偏序集，就相當於我們目前「已知」的所有資訊。一個超級粗略的估計如下：\(n\) 個元素的 Poset 至多有 \(2^{n^2-n}\) 種<sup class="footnote-reference"><a href="#1">1</a></sup>，因為這個集合當中、任何兩個元素之間的關係要嘛可以出現、要嘛不能出現。
由於我們今天舉的例子是 \(n=12\)，讓我先把 \(n=12\) 筆資料的 Poset 數量列出來提供參考：\(414864951055853499\)。總之是個有限的數字！</p>
<p>要如何找出排序 \(n=12\) 筆資料的最小排序方法數呢？我們可以利用 <strong>動態規劃</strong> 的概念來解它！
對於任何一個偏序集 \(P\)，定義 \(dp(P)\) 表示完成排序尚需要的最少比較次數。
不難發現，一開始我們掌握的資訊量是一個空的偏序集 \(P_\emptyset\)，所求的最少比較次數 \(S(12) = dp(P_\emptyset)\)。</p>
<p>假設我們現在手上有個偏序集 \(P_0\)，第一步該做什麼呢？我們可以選擇任何兩筆資料 \(x_i, x_j\) 進行比較。無論獲得什麼結果，我們會得到另一個偏序集 \(P_1 = P_0(x_i \lessgtr x_j)\)。注意到，如果 \(x_i\) 與 \(x_j\) 在 \(P_0\) 之中不存在任何關聯，那麼新的偏序集的大小必定大於原本的 \(|P_1\mid  &gt; \mid P_0\mid \)。
如果我們事先計算好，對所有 \(i, j\)，到底產生出來的偏序集，其最糟情形下會需要幾次比較。選取最小的那組 \((i, j)\) 就可以了！寫成遞迴關係會變成：</p>
<p>\[
dp(P_0) = 1 + \min_{i, j} \left( \max\{dp(P_0(x_i &lt; x_j)), dp(P_0(x_i &gt; x_j))\} \right)
\]</p>
<p>對於 \(n=12\) 筆資料而言，我們就獲得一個 \(n^2\times 414864951055853499\) 時間複雜度的動態規劃演算法了！真是好棒棒。這個數字可能有點大，如果我們只考慮<strong>同構（Isomorphic）</strong> 的偏序集，那這個數字可以降到 \(n^2\times 1104891746\)，感覺變得可以負擔了。在這邊「同構」是這樣定義的：把輸入的資料 ID 隨意亂序置換以後，產生的所有偏序集都會被視為同構。顯然這些偏序集距離完全排好順序所需要的比較次數都相同，一旦計算出其中一個偏序集的 \(dp\) 值，就等同於計算出了所有其他同構的偏序集的 \(dp\) 值。</p>
<a class="header" href="#有向無環圖-directed-acyclic-graph" id="有向無環圖-directed-acyclic-graph"><h2>有向無環圖 Directed Acyclic Graph</h2></a>
<p>在我們要討論的主題中，偏序集與有向無環圖（Directed Acyclic Graph, DAG）其實有著非常相似的概念：
我們可以利用 DAG 來表達一個 Poset \(P\)，考慮一個圖 \(G_P\)，其頂點集合就是所有的 \(n\) 筆資料 \(x_1, x_2, \ldots, x_n\)。只要在 Poset 裡面 \(x_i &lt; x_j\)，我們就加上一條 \(x_i\to x_j\) 的有向邊（arc）。</p>
<p>如果這 \(n\) 筆資料是完全排序的，那麼對應到的圖 \(G_P\) 會是一個完全圖、而且它唯一的<strong>拓撲排序 Topological Order</strong> 就是把所有資料排序後的結果。
藉由圖論，我們可以舉一個 \(n=5\) 的例子，把上面的動態規劃解釋得更美觀一點～</p>
<p><img src="./poset-efficiency0.png" alt="" /></p>
<hr />
<p>明天來繼續講偏序集與完全排序之間的更多關係！</p>
<a class="header" href="#延伸閱讀" id="延伸閱讀"><h3>延伸閱讀</h3></a>
<ul>
<li><a href="https://luciuschang.wordpress.com/2016/10/24/%E6%95%B8%E5%AD%B8%E5%A6%82%E4%BD%95%E5%AE%9A%E7%BE%A9%E3%80%8C%E9%97%9C%E4%BF%82%E3%80%8D/">張志鴻教授：數學如何定義「關係」</a></li>
</ul>
<div class="footnote-definition" id="1"><sup class="footnote-definition-label">1</sup>
<p>\(n\) 筆資料的偏序集計數 OEIS A001035: https://oeis.org/A001035</p>
</div>
<div class="footnote-definition" id="2"><sup class="footnote-definition-label">2</sup>
<p>把同構的偏序集收起來以後的計數 OEIS A000112: https://oeis.org/A000112</p>
</div>
<!--
\tikzset{n/.style={inner sep=0, minimum size=4pt, circle, fill=black}}
dp\left(
\tikz[baseline={([yshift=-.5ex]current bounding box.center)}, scale=0.7, thick]{
\node[n](A) at (0, 0) {};
\node[n](B) at (1, 0) {};
\node[n](C) at (2, 0) {};
\node[n](D) at (0.5, -0.7) {};
\node[n](E) at (1.5, -0.7) {};
\draw (A) edge[-latex] (B);
\draw (B) edge[-latex] (C);
\draw (E) edge[-latex] (C);
}
\right)
= 
1 + \max\left\{
dp\left(
\tikz[baseline={([yshift=-.5ex]current bounding box.center)}, scale=0.7, thick]{
\node[n](A) at (0, 0) {};
\node[n](B) at (1, 0) {};
\node[n](C) at (2, 0) {};
\node[n](D) at (0.5, -0.7) {};
\node[n](E) at (1.5, -0.7) {};
\draw (A) edge[-latex] (B);
\draw (B) edge[-latex] (C);
\draw (E) edge[-latex] (C);
\draw (D) edge[-latex, color=red] (B);
}
\right),
dp\left(
\tikz[baseline={([yshift=-.5ex]current bounding box.center)}, scale=0.7, thick]{
\node[n](A) at (0, 0) {};
\node[n](B) at (1, 0) {};
\node[n](C) at (2, 0) {};
\node[n](D) at (1.5, +0.7) {};
\node[n](E) at (1.5, -0.7) {};
\draw (A) edge[-latex] (B);
\draw (B) edge[-latex] (C);
\draw (E) edge[-latex] (C);
\draw (B) edge[-latex, color=blue] (D);
}
\right)
\right\}
--><a class="header" href="#偏序集排序二" id="偏序集排序二"><h1>偏序集排序（二）</h1></a>
<p>對於一個偏序集 \(P\)，我們可以利用動態規劃的方法，找出從 \(P\) 的狀態開始排序，到完整排序時，最壞情形下，至少需要幾次比較。
可惜的是，這個方法需要跑遍所有可能的 \(n\) 個點的偏序集，而這個數量是隨著 \(n\) 越大而呈現指數級別成長的<sup class="footnote-reference"><a href="#1">1</a></sup>。</p>
<a class="header" href="#線性延伸-linear-extensions" id="線性延伸-linear-extensions"><h2>線性延伸 Linear Extensions</h2></a>
<p>有沒有更有效的方法來提早排除某些「需要太多次比較」的偏序集呢？
其實是可以的，給定一個偏序集 \(P\)，對於任何一個所有資料 \(x_1, x_2, \ldots, x_n\) 的全排列 \(\sigma\)，我們說 \(\sigma\) 是 \(P\) 的一個<strong>線性延伸（Linear Extension）</strong>，若且唯若對於任兩筆資料 \(x_i\) 與 \(x_j\)，一旦在 \(P\) 上面 \(x_i &lt; x_j\) \(\implies\) 在 \(\sigma\) 之中 \(x_i\) 出現在 \(x_j\) 前面。
如果我們用圖論的語言來描述的話，其實滿足條件的 \(\sigma\) 就會是 \(P\) 所對應的有向無環圖 \(G_P\) 上頭的一個拓撲排序。</p>
<p>我們用 \(e(P)\) 來表示偏序集上面的線性延伸數量，當我們寫成 \(e(G)\) 的時候，也代表著一個有向圖 \(G\) 中拓撲排序的方法數。兩者定義雖然不太一樣，在這裡我們不妨就混著使用了。
舉例而言，如果 \(P_{\rm{total}}\) 是一個全序集，那麼它有唯一的線性延伸，因此 \(e(P_{\rm{total}}) = 1\)。如果 \(P_\emptyset\) 是空序集，那麼任何一個排列都會是 \(P_\emptyset\) 的線性延伸，此時 \(e(P_\emptyset) = n!\)。</p>
<p>還記得資訊理論給出的複雜度下界嗎？同樣的方法在這邊也適用！</p>
<a class="header" href="#性質-23" id="性質-23"><h3>性質 23</h3></a>
<p>對於一個偏序集 \(P\)，進行兩兩比較方式排序，至少需要 \(\ceil{\log e(P)}\) 次比較。</p>
<a class="header" href="#排序效率-efficiency" id="排序效率-efficiency"><h2>排序效率 Efficiency</h2></a>
<p>從一個 \(n\) 個點的偏序集 \(P\) 開始，假設我們把 \(x_i\) 與 \(x_j\) 拿來互相比較，比完以後會得到 \(P_&lt; := P(x_i &lt; x_j)\)、或是 \(P_&gt; := P(x_i &gt; x_j)\) 這兩種可能。
顯然對於所有 \(P\) 的線性延伸，都會恰好是 \(P_&lt;\) 或 \(P_&gt;\) 的線性延伸，於是我們有
\[
\label{eq}
e(P) = e(P_&lt;) + e(P_&gt;)\text{。} \tag{*}
\]</p>
<p>如果 \(e(P_&lt;) \neq e(P_&gt;)\)，那麼邪惡的生測資者會設計測試資料，讓你落入線性延伸數量比較大的那種情形，然後你就會「讓剛才花費的 1 次比較，得不到 1 bit 的資訊量」。換句話說，我們想像中可能會出現「效率流失」的情形。
Knuth 的書中<sup class="footnote-reference"><a href="#2">2</a></sup>寫道：黃光明教授(Frank Hwang) 與林甡(Shen Lin) 定義了<strong>排序效率（Efficiency）</strong>：假設從一個空序集經過了 \(k\) 次詢問後，得到一個偏序集 \(P\)。那麼我們定義其排序效率 Efficiency 為
\[ E(P) = \frac{n!}{2^k e(P)} \]
這個式子可以解釋為：理論上經過 \(k\) 次比較，我們應該要能將 \(n!\) 種排序可能縮小至 \(1/2^k\) 倍數，但事實上剩存的排列數 \(e(P)\) 可能仍然比 \(n!/2^k\) 來得大。我們就用這個數值來量化到底到目前為止的 \(k\) 次比較有沒有效率。</p>
<p>從上面打星號的 \(\eqref{eq}\) 來看，我們知道經過一次比較後，比較糟的那個情形，其排序效率不超過當前的排序效率：
\[
E(P) \ge \min\{ E(P_&gt;), E(P_&lt;) \}
\]</p>
<p>這個定義可以怎麼幫助我們理解排序呢？假設我們的目標，是要在 \(7\) 次比較之內，排列 \(n=5\) 筆資料。此時我們可以事先估計在 \(7\) 次比較之後，得到全序集當下的排序效率：
\[
E(P_{\rm{total}}) = 5!/2^7 = 120/128 = 15/16\text{。}
\]
這告訴我們什麼？在任何的情況下，我們不應該讓排序效率降低至 \(&lt;\frac{15}{16}\)。換句話說，在選擇下一個要拿來比較的 \(x_i, x_j\) 的時候，我們只需要考慮那些「得出排序結果後，效率仍保持在 \(\ge 15/16\) 的那些 \((x_i, x_j)\) 配對」。</p>
<p>如果不存在這樣的配對，我們便證明了不存在任何基於比較的排序法，能夠在 \(7\) 次之內完成排序。（事實上可以做到 \(7\) 次比較，請參考前幾日的<a href="sorting/./minimum-comparison-sort.html">最少比較排序</a>）</p>
<p>這個工具可以幫助我們排除 \(n=12\) 的時候需要進行的動態規劃狀態數！
根據 Knuth 書中的解釋，經過一連串更深入的分析──我們需要兩件事情：
一、只需要考慮所有連通的、不超過 \(n=12\) 個點的圖 \(G\)，然後每一次比較要嘛在圖上加一條邊、要嘛把兩個圖用一條邊合併起來。二、找到一個方式估計出 \(e(P)\) 的值（顯然無法有效率地直接計算它），其排除效率小於 \(12!/2^{29} \approx 0.89221\) 的偏序集就可以從昨天提到的 1104891746 降到 1649 了！</p>
<p>(tl; dr) 最後的結論就是，在展開所有排序效率 \(\ge 0.89221\) 的搜索狀態時，我們始終無法達到全序集。因此可以證得 \(S(12) &gt; 29\)，由 Ford-Johnson 演算法可得知 \(F(12)=30\)。因此得證 \(S(12)=30\)。</p>
<hr />
<p>如果給定了偏序集 \(P\)，雖然無法精準計算出所有的線性延伸數量 \(e(P)\)，我們有沒有其他方式來得出 \(e(P)\) 的估計呢？</p>
<a class="header" href="#備註-1" id="備註-1"><h3>備註</h3></a>
<p>Linear Extension 在紡織科學裡面會被翻譯成直線伸長唷…</p>
<p><img src="./poset-efficiency1.png" alt="" /></p>
<a class="header" href="#延伸閱讀-1" id="延伸閱讀-1"><h3>延伸閱讀</h3></a>
<ul>
<li><a href="https://web.math.sinica.edu.tw/math_media/d381/38103.pdf">黃光明教授：我與數學研究的第一次邂逅</a></li>
</ul>
<div class="footnote-definition" id="1"><sup class="footnote-definition-label">1</sup>
<p>可以用 \(n\) 個點的無標號、無方向但有根樹來作下界估計，根據 <a href="https://oeis.org/A000081">OEIS A000081</a>，這個數字至少是 \(0.440\times 2.956^{n} \times n^{-5/2}\)，成長迅速。</p>
</div>
<div class="footnote-definition" id="2"><sup class="footnote-definition-label">2</sup>
<p>Donald Knuth, <em>The Art of Computer Programming</em>, Volumn 3, Page 188-190.</p>
</div>
<a class="header" href="#偏序集排序三" id="偏序集排序三"><h1>偏序集排序（三）</h1></a>
<p>今天的目標是要試圖找出，計算一個偏序集 \((P, &lt;)\) 上面線性延伸數量 \(e(P)\) 的估計。
如果有好辦法得到近似值，就可以拿它來計算排序效率、甚至可以用它來判斷每一次要比較哪兩個元素比較好。</p>
<a class="header" href="#順序多胞形-order-polytope" id="順序多胞形-order-polytope"><h2>順序多胞形 Order Polytope</h2></a>
<p>今天的主角是一個在 \(n\) 維空間中的多胞形：\(\mathcal{O}(P)\)。
更精確地說，它是一個被包含在 \(n\) 維空間中單位立方體內的一個多胞形。
我們不妨假設 \(P\) 這個集合的元素有 \(x_1, x_2, \ldots, x_n\)。
而任何一個 \(P\) 的線性延伸 \(\sigma\) 可以表示成一個註標 \(\{1, 2, \ldots, n\}\) 的一個排列
\((\sigma(1), \sigma(2), \ldots, \sigma(n))\)，使得 \(x_{\sigma(i)} &lt; x_{\sigma(j)}\) \(\implies\) \(i &lt; j\)。（由於是偏序集，逆命題並不一定成立。）</p>
<p>這個多胞形 \(\mathcal{O}(P)\) 的定義是這樣的：</p>
<p>\[
\mathcal{O}(P) = \{ (z_1, z_2, \ldots, z_n) \ \mid \ z_i &lt; z_j \text{ whenever } x_i &lt; x_j \text{ in } P \}
\]</p>
<a class="header" href="#定理-24" id="定理-24"><h3>定理 24</h3></a>
<p>\(\mathcal{O}(P)\) 是一個凸多胞形，而且其體積恰好等於 \(e(P)/n!\)。</p>
<a class="header" href="#證明-15" id="證明-15"><h3>證明</h3></a>
<p>要證明 \(\mathcal{O}(P)\) 是凸多胞形，只需要說明對任何兩個 \(\mathcal{O}(P)\) 裡面的點 \(z, z'\)，其連線中點 \((z+z')/2\) 仍然落在 \(\mathcal{O}(P)\) 裡面即可。根據定義，每一個點都會滿足至少一個對應的 \(P\) 的線性延伸。假設 \(z\) 對應的是 \(\sigma\)、\(z'\) 對應的是 \(\sigma'\)，顯然對於所有 \(P\) 中的大小順序 \(x_i &lt; x_j\)，都有 \(z_{\sigma^{-1}(i)} \le z_{\sigma^{-1}(j)}\)、且 \(z'_{\sigma^{-1}(i)} \le z'_{\sigma^{-1}(j)}\)。因此加起來除以 \(2\) 以後，將所有座標值由小到大排列得到的新的排列 \(\sigma''\) 仍然是 \(P\) 的一個線性延伸。</p>
<p>關於體積的部分，我們考慮任何一個線性延伸 \(\sigma\)。由單一線性延伸定義出來的多胞形，它會是一個<strong>單純形（Simplex）</strong>。這邊定義出來的單純形，經過座標置換以後，可以發現其體積總是等價於以下這個單純形的體積：</p>
<p>\[
\Delta_*^n := \{(z_1, z_2, \ldots, z_n)\in \mathbb{R}^n \ \mid \ 0 \le z_1 \le z_2 \le \cdots \le z_n \le 1\}
\]</p>
<p>這個體積是多少呢？你可以用積分方法求出、也可以用對稱方法求出：總共有 \(n!\) 種排列、每一個排列定義出來的單純形，體積都相同，而且它們全部聯集起來恰好組成 \([0, 1]^n\subset \mathbb{R}^n\) 的小方塊，也就是說他們體積總和恰好等於 \(1\)。一個這樣的單純形的體積是 \(1/n!\)。於是，\(\mathcal{O}(P)\) 的體積就恰好等於 \(e(P)/n!\) 啦～
<span style="float:right">\(\square\)</span></p>
<p>我們之前隱約提及，要計算 \(e(P)\) 的精確值是困難的。而這也等價於，要精確計算出 \(\mathcal{O}(P)\) 的體積，也是困難的。
幸虧，1989 年的時候 Dyer, Frieze, 與 Kannan<sup class="footnote-reference"><a href="#1">1</a></sup><sup class="footnote-reference"><a href="#3">2</a></sup> 提出了一個多項式隨機演算法，能夠有效率地計算出近似值。但他們的方法都是基於 random walk 的，基本的想法是從任何一個 linear extension 開始，利用 random walk 依照某個特定機率分布交換某兩個沒有關聯的元素。後面的分析就精彩了，這邊先暫時留給有興趣的朋友閱讀吧，有機會我會再補上證明的。</p>
<p>最後我們再描述一個關於凸多胞形的性質。</p>
<a class="header" href="#ehrhart-多項式" id="ehrhart-多項式"><h2>Ehrhart 多項式</h2></a>
<p>法國數學家 <a href="https://en.wikipedia.org/wiki/Eug%C3%A8ne_Ehrhart">Eugène Ehrhart</a> 在中學教了十多年的書以後，在六十歲的時候終於獲得了博士學位。在那之前，他於 1962 年提出了皮克公式（Pick's Theorem）的 \(N\) 維加強版：<a href="https://en.wikipedia.org/wiki/Ehrhart_polynomial">Ehrhart Polynomial</a>。</p>
<p>故事是這樣的：假設我們有一個 \(N\) 維度的多胞形 \(P\)，如果把每一個座標軸都伸長 \(k\) 倍，那基本上體積也會跟著放大 \(k^N\) 倍，對吧？如果這時候我們計較的是這個被放大的多胞形 \(kP\)，其邊界或內部的「整數座標點」的數量 \(L(P, k)\)。Ehrhart 證明出了它會形成一個關於 \(k\) 的 \(N\) 次多項式！</p>
<p>不意外地，\(L(P, k)\) 的首項係數（最高次項 \(k^N\) 的係數）就會是這個多胞形真正的體積。
這個 Ehrhart 給我們有趣的證明技巧：如果我們要證明兩個多胞形體積相同，我們可以透過證明其 Ehrhart 相等的手法來證明其體積相等。</p>
<hr />
<p>明天我們來介紹另一種比較平易近人的、計算出 \(e(P)\) 近似值的方式。
這個方式是透過 1986 年 Richard P. Stanley<sup class="footnote-reference"><a href="#2">3</a></sup> 提出的趣味觀察，把這個順序多胞形與另一個直鏈和多胞形（Chain Polytope，暫譯直鏈和多胞形) 連結在一起，進而從解另一個多胞形上的凸函數優化（Convex Optimization）來達到計算近似 \(e(P)\) 的目的。</p>
<a class="header" href="#延伸閱讀-2" id="延伸閱讀-2"><h3>延伸閱讀</h3></a>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Order_polytope">維基百科：Order Polytope</a></li>
<li>https://www.math.sci.hokudai.ac.jp/~wakate/mcyr/2016/pdf/TSUCHIYA.pdf</li>
<li>https://math.dartmouth.edu/~pw/M100W11/seth.pdf</li>
<li><a href="https://ccjou.wordpress.com/2013/05/20/%E5%A4%9A%E8%83%9E%E5%BD%A2/">線代啟示錄：多胞形</a></li>
</ul>
<div class="footnote-definition" id="1"><sup class="footnote-definition-label">1</sup>
<p>Martin Dyer, Alan Frieze, and Ravi Kannan, _A Random Polynomial Time Algorithm for Approximating the Volume of Convex Bodies. <a href="https://www.math.cmu.edu/%7Eaf1p/Texfiles/oldvolume.pdf">https://www.math.cmu.edu/~af1p/Texfiles/oldvolume.pdf</a></p>
</div>
<div class="footnote-definition" id="2"><sup class="footnote-definition-label">3</sup>
<p>Richard P. Stanley, <em>Two Poset Polytopes</em>, 1986. <a href="http://dedekind.mit.edu/%7Erstan/pubs/pubfiles/66.pdf">http://dedekind.mit.edu/~rstan/pubs/pubfiles/66.pdf</a></p>
</div>
<div class="footnote-definition" id="3"><sup class="footnote-definition-label">2</sup>
<p>Graham Brightwell and Peter Winkler, <em>Counting Linear Extensions</em>, 1991. <a href="https://link.springer.com/content/pdf/10.1007/BF00383444.pdf">https://link.springer.com/content/pdf/10.1007/BF00383444.pdf</a></p>
</div>
<a class="header" href="#偏序集排序四" id="偏序集排序四"><h1>偏序集排序（四）</h1></a>
<p>為了繞過使用深奧隨機走訪方法與馬可夫鏈而得到的估計 \(e(P)\) 近似演算法，
我們今天把這個順序多胞形轉換一下，變成另一個直鏈和多胞形（Chain Polytope）。</p>
<a class="header" href="#直鏈和多胞形-chain-polytope" id="直鏈和多胞形-chain-polytope"><h2>直鏈和多胞形 Chain Polytope</h2></a>
<p>在 poset \((P, &lt;)\) 上面的一條<strong>直鏈（chain）</strong> \(C\subset P\)，其實就是一個任兩元素都可以比較的子集合。
我們考慮以下的<strong>直鏈和多胞形（chain polytope）</strong>：</p>
<p>\[
\mathcal{C}(P) = \{ (z_1, \ldots, z_n) \ \mid \ \forall \text{ chain } C, 0\le \sum_{x_i\in C} z_i \le 1 \}
\]</p>
<p>換句話說，只要 \(n\) 個介於 \([0, 1]\) 之間的實數，滿足對於 poset 之中任何一條直鏈，它對應數字的總和不超過 \(1\) 的話，這個點就會被我們加入多胞形之中。</p>
<a class="header" href="#兩個多胞形之間的轉換" id="兩個多胞形之間的轉換"><h2>兩個多胞形之間的轉換</h2></a>
<p>要怎麼找出直鏈和多胞形 \(\mathcal{C}(P)\) 與順序多胞形 \(\mathcal{O}(P)\) 之間的關聯呢？
試想像一下，如果我們有一條鏈：</p>
<p><img src="./chain-polytope-and-graph-entropy1.png" alt="" /></p>
<!--
\tikzset{n/.style={inner sep=0pt, minimum size=4pt, circle, fill=black}}
\tikz{
\foreach \x/\c in {1/0.1,2/0.3,3/0.05,4/0.2,5/0.05,6/0.3} {
  \node[n,label=below:{\\(x\_\x\\)}](A\x) at (\x, 0) {};
  \node[rotate=30,anchor=west,shift={(0.1, 0.1)}] at (A\x) {\c};
};
\foreach \x in {1,2,...,5} {
\pgfmathsetmacro\y{\x+1}%
\draw (A\x) edge [-latex,thick] (\\((A\y)+(-0.1,0)\\));
};
}
-->
<p>那麼 \((0.1, 0.3, 0.05, 0.2, 0.05, 0.3, \ldots)\in \mathcal{C}(P)\)。由於在 \(\mathcal{C}(P)\) 上頭，每一條鏈上面的數值總和都不超過 \(1\)，透過 <strong>前綴和（Prefix Sum）</strong> 的概念，我們可以定義出一條非遞減的序列如下：</p>
<p><img src="./chain-polytope-and-graph-entropy2.png" alt="" /></p>
<!--
\tikzset{n/.style={inner sep=0pt, minimum size=4pt, circle, fill=black}}
\tikz{
\foreach \x/\c in {1/0.1,2/0.4,3/0.45,4/0.65,5/0.7,6/1.0} {
  \node[n,label=below:{\\(x\_\x\\)}](A\x) at (\x, 0) {};
  \node[rotate=30,anchor=west,shift={(0.1, 0.1)},color=blue] at (A\x) {\c};
};
\foreach \x in {1,2,...,5} {
\pgfmathsetmacro\y{\x+1}%
\draw (A\x) edge [-latex,thick] (\\((A\y)+(-0.1,0)\\));
};
}
-->
<p>每一個點的數值變成了這條鏈上面它所有祖先原本的數值總和，加上自己本身的數值。
不難發現，現在這個賦值方式滿足順序多胞形的條件：
\[
(0.1, 0.4, 0.45, 0.65, 0.7, 1.0, \ldots) \in \mathcal{O}(P)
\]</p>
<p>現在讓我們來考慮推廣版的情形：如果一個點屬於超過一條鏈怎麼辦呢？對於一個點 \(z=(z_1, \ldots, z_n)\in\mathcal{C}(P)\)，我們可以定義出一個點 \(\varphi(z) = (z'_1, \ldots, z'_n)\in\mathcal{O}(P)\)，其中 \(z'_i = z_i + \max_{j: x_j &lt; x_i} z'_j\)。這個轉換 \(\varphi\) 其實也是一一對應的：對於一個點 \(z' = (z'_1, \ldots, z'_n)\in\mathcal{O}(P)\)，我們可以回推出 \(\varphi^{-1}(z') = (z_1, \ldots, z_n)\in\mathcal{C}(P)\)，使得 \(z_i = z'_{i} - \max_{j: x_j &lt; x_i} z'_j\)。</p>
<p>因此我們可以說：對於任意偏序集，都存在一個雙射變換 \(\varphi_P: \mathcal{C}(P)\to \mathcal{O}(P)\)。可惜的是，光憑這個結論，我們無法斷定這兩個多胞形的體積關係。這時候，就是我們前一篇介紹的 Ehrhart 多項式派上用場的時候啦！</p>
<a class="header" href="#定理-25-stanley-1986sup-classfootnote-referencea-href11asup" id="定理-25-stanley-1986sup-classfootnote-referencea-href11asup"><h3>定理 25 [Stanley 1986]<sup class="footnote-reference"><a href="#1">1</a></sup></h3></a>
<p>對於任何偏序集。\(\mathcal{C}(P)\) 的體積與 \(\mathcal{O}(P)\) 的體積相等。</p>
<a class="header" href="#證明-16" id="證明-16"><h3>證明</h3></a>
<p>還記得我們說過多胞形的 Ehrhart 多項式嗎？如果把 \(\mathbb{R}^n\) 空間中，每一個座標軸放大整數倍 \(k\)，那麼多胞形內部的格子點（所有座標都是整數的點）的數量會以 \(\Theta(k^n)\) 的速度成長，而這個點數可以被一個 \(n\) 次多項式表示之。</p>
<p>換句話說，如果我們有辦法證明 \(\mathcal{C}(P)\) 與 \(\mathcal{O}(P)\)，在座標軸放大任意整數倍之後，格子點數仍然相同，那麼很顯然它們有一模一樣的 Ehrhart 多項式（任何一個 \(n\) 次單變數多項式可以被 \(n+1\) 個取值唯一決定。）</p>
<p>而這個整數點格子數量的結論顯然是正確的：在放大 \(k\) 倍之後，若 \(kz = (kz_1, kz_2, \ldots, kz_n)\) 是格子點，那麼根據 \(\varphi\) 的定義，在變換之後 \(\varphi(kz) = kz'\) 也會是格子點。反之亦然。於是，我們就得證啦～</p>
<hr />
<p>這個直鏈和多胞形有什麼好處呢？它其實有另一個等價的定義：如果我們從偏序集 \(P\) 當中，任何兩個可以比較的元素對，都建立一條邊，我們會得到一個無向圖 \(G(P)\)。\(G(P)\) 也被稱為 \(P\) 的<strong>可比較圖（Comparability Graph）</strong>。</p>
<p>而 \(\mathcal{C}(P)\) 呢，它會恰好等於所有 \(G(P)\) 上面所有「獨立集（stable set, independent set）」所對應到的單位向量，與原點形成的凸組合（convex combination）空間。</p>
<p>明天我們來看看這個 \(G(P)\) 的補圖 \(\overline{G(P)}\) <strong>不可比圖（incomparability graph）</strong>，他對於 \(e(P)\) 的估計有什麼厲害的幫助吧！</p>
<div class="footnote-definition" id="1"><sup class="footnote-definition-label">1</sup>
<p>Richard P. Stanley, <em>Two Poset Polytopes</em>, 1986. <a href="http://dedekind.mit.edu/%7Erstan/pubs/pubfiles/66.pdf">http://dedekind.mit.edu/~rstan/pubs/pubfiles/66.pdf</a></p>
</div>
<a class="header" href="#偏序集排序五" id="偏序集排序五"><h1>偏序集排序（五）</h1></a>
<p>前一次我們提到了，從一個偏序集 \((P, &lt;)\) 定義出來的直鏈和多胞形（Chain Polytope）\(\mathcal{C}(P)\)，其體積與順序多胞形（Order Polytope）\(\mathcal{O}(P)\) 相等，且剛好等於 \(e(P) / n!\)。</p>
<p>但是計算一個任意多胞形的確切體積，是困難的。幸好，如果我們只需要估計 \(e(P)\) 的值（甚至可以放寬條件、只需要估計 \(\log e(P)\) 的值就行了），不需要計算出整個多胞形的確切體積。</p>
<a class="header" href="#利用長方體估算體積" id="利用長方體估算體積"><h2>利用長方體估算體積</h2></a>
<p>能夠簡單計算體積的形狀，大概就是長方體了吧。
透過觀察，我們發現，對於任何一個直鏈和多胞形內的點 \(z=(z_1, z_2, \ldots, z_n) \in\mathcal{C}(P)\)，都可以定義出一個長方體，其兩個對角點剛好分別是原點 \((0, 0, \ldots, 0)\) 以及 \(z\)。
換句話說，這個長方體
\[
\text{Box}(z) = [0, z_1]\times [0, z_2]\times \cdots \times [0, z_n]\subset \mathcal{C}(P)
\]</p>
<p>\(\text{Box}(z)\) 的體積，顯然可以直接寫出來：\(z_1z_2\cdots z_n\)。既然長方體的體積如此好算，那麼我們試圖找出 \(\mathcal{C}(P)\) 內部最大的長方體，用它來估計 \(e(P)\) 的值，應該很有希望。於是，我們可以寫下一個最佳化問題：</p>
<p>\[
\begin{aligned}
\text{maximize }\ \ &amp; z_1z_2\cdots z_n\\
\text{subject to }\ \ &amp; z\in \mathcal{C}(P)
\end{aligned}
\]</p>
<p>這個問題，乍看之下有點棘手，理由是，目前我們能夠輕鬆解決的問題，大多是線性規劃、或是凸函數最佳化等等。
而上述題目，有兩個地方跟我們平常解的最佳化問題有些出入：其一是目標函數，全部乘在一起了看起來有點麻煩；其二是變數條件，\(\mathcal{C}(P)\) 雖然是個多胞形，能夠被許多線性條件表示。
但是， \(\mathcal{C}(P)\) 的定義並非如 \(\mathcal{O}(P)\) 一般可以單純以 \(z_i \le z_j \ \ \forall x_i, x_j\in P, x_i &lt; x_j\) 的條件來表示之。
最直接的寫法──枚舉所有最大獨立集，並設定所有參與獨立集裡面的變數總和不超過 1 ──可能會需要指數級別數量的線性不等式。</p>
<p>不過呢，如果我們把目標函數取個 \(-\log\)，然後除個 \(n\) 平均一下，就可以讓目標函數變成凸函數啦～於是第一個問題變得可解一些：</p>
<p>\[
\begin{aligned}
\text{minimize }\ \ &amp; H(z) := -\frac{1}{n}\sum_{i=1}^n \log z_i \\
\text{subject to }\ \ &amp; z\in \mathcal{C}(P)
\end{aligned}
\]</p>
<p>（備註：上面的 \(1/n\) 並不影響最佳化的解，加上去是因為這個東西根本一臉資訊理論中的熵 entropy 的定義，為了保持定義的一致性加上去的。）
要解決第二個問題，我們其實可以利用 membership oracle 的觀念：拿來解線性規劃的工具，如橢球法（Ellipsoid Method）與內點法（Interior Point Method），都只需要判斷「目前某個點是否滿足指定條件」的工具就行了。也就是說，我們可以透過兩個多胞形之間的轉換 \(\varphi^{-1}\)，將欲判斷 membership 的點從 \(\mathcal{C}(P)\) 換到 \(\mathcal{O}(P)\) 去檢查（或等價地、乾脆直接在 poset 上面作一次動態規劃），就可以在 \(O(n^2)\) 時間內判斷欲查詢的點是否在 \(\mathcal{C}(P)\) 內部囉。</p>
<p>於是，我們可以套用凸函數最佳化的大刀，幫助我們找出直鏈和多胞形內部最大的長方體<sup class="footnote-reference"><a href="#1">1</a></sup>。
假設這個最佳化問題的解是 \(z^*\)，對應到的長方體體積為 \(\text{vol}(\text{Box}(z^*)) = 2^{-nH(z^*)}\)。為了方便起見，我們暫且把這個體積記作 \(2^{-nH(P)}\)。於是呢，可以得出以下結論：</p>
<a class="header" href="#引理-26" id="引理-26"><h3>引理 26</h3></a>
<p>\[
2^{-nH(P)} \le e(P)/n!
\]</p>
<p>整理一下可得</p>
<p>\[
\log e(P) + nH(P) \ge \log n!
\]</p>
<p>這告訴我們什麼呢？當這個解 \(nH(P)\) 的值越小的時候，\(\log e(P)\) 越大（存在更多的線性延伸）。等價地地，若 \(\log e(P)\) 很小（只有少許的線性延伸），那麼 \(nH(P)\) 保證很大。
說到這裡，筆者不禁捫心自問，原本不是想估計 \(e(P)\) 的值嗎？為什麼我們可以只關心 \(\log e(P)\)？</p>
<a class="header" href="#主題偏序集排序" id="主題偏序集排序"><h2>主題──偏序集排序</h2></a>
<p>原因是這樣的：如果給定了部分資料之間的大小關係 \(P\)，顯然從前些日子提到的 \(S(12)&gt;29\) 來看，存在一些偏序集無法在恰好 \(\ceil{\log e(P)}\) 次比較之內排好所有資料的順序。那麼，我們想知道的是，能否退而求其次，如同大多數使用 \(O(n\log n)\) 次比較的排序演算法，允許常數倍數的誤差，在 \(O(\log e(P))\) 次比較之內排好順序呢？</p>
<p>也就是說，如果這個偏序集 \(P\) 已經幾乎排好序了，有沒有比 \(\Theta(n\log n)\) 使用更少次數的排序演算法，可以正確地利用少少的 \(O(\log e(P))\) 次比較，從而達到正確排序呢？
答案是有的<sup class="footnote-reference"><a href="#2">2</a></sup>，我們可以利用這幾天討論過的觀念，一步一步邁向 \(O(\log e(P))\) 的偏序集排序。我們甚至還不需要真的用大刀解凸函數最佳化問題，只需要觀念就可以了。</p>
<p>一個有趣的暖身是這樣的：從上面的引理 26，我們可以得到一個超簡單 \(O(\log n\cdot \log e(P))\) 的偏序集排序演算法。大家也不妨猜猜看！這個我們明天講～</p>
<hr />
<div class="footnote-definition" id="1"><sup class="footnote-definition-label">1</sup>
<p>至於大刀該怎麼用比較正確，詳情以後再談好了…。</p>
</div>
<div class="footnote-definition" id="2"><sup class="footnote-definition-label">2</sup>
<p>Jean Cardinal, Samuel Fiorini, Gwenaël Joret, Raphaël M. Jungers, J. Ian Munro, <em>Sorting under Partial Information (without the Ellipsoid Algorithm)</em>, Combinatorica 33, 655–697 (2013). https://doi.org/10.1007/s00493-013-2821-5, <a href="https://arxiv.org/pdf/0911.0086.pdf">ArXiv</a>.</p>
</div>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        

        
        <!-- Google Analytics Tag -->
        <script type="text/javascript">
            var localAddrs = ["localhost", "127.0.0.1", ""];

            // make sure we don't activate google analytics if the developer is
            // inspecting the book locally...
            if (localAddrs.indexOf(document.location.hostname) === -1) {
                (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
                (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
                })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

                ga('create', 'UA-68887724-4', 'auto');
                ga('send', 'pageview');
            }
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        
        <script type="text/javascript" src="mathjax-config.js"></script>
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>
        
        

    </body>
</html>
